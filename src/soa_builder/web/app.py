"""FastAPI web application for interactive Schedule of Activities creation.

Endpoints:
  POST /soa {name} -> create SOA container
  GET /soa/{id} -> summary
  POST /soa/{id}/visits {name, raw_header} -> add visit
  POST /soa/{id}/activities {name} -> add activity
  POST /soa/{id}/cells {visit_id, activity_id, status} -> set cell value
  GET /soa/{id}/matrix -> returns visits, activities, cells matrix
  GET /soa/{id}/normalized -> run normalization pipeline and return summary

Data persisted in SQLite (file: soa_builder_web.db by default).
"""

from __future__ import annotations

import os
import sqlite3
import csv
import tempfile
import json
from fastapi import FastAPI, HTTPException, Request, Form, UploadFile, File, Response
import re
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
from datetime import datetime, timezone
import io
import pandas as pd
from ..normalization import normalize_soa
import requests, time, logging
from dotenv import load_dotenv
import re as _re

load_dotenv()  # must come BEFORE reading env-based configuration so values are populated
DB_PATH = os.environ.get("SOA_BUILDER_DB", "soa_builder_web.db")
NORMALIZED_ROOT = os.environ.get("SOA_BUILDER_NORMALIZED_ROOT", "normalized")


def _get_cdisc_api_key():
    return os.environ.get("CDISC_API_KEY")


def _get_concepts_override():
    return os.environ.get("CDISC_CONCEPTS_JSON")


_concept_cache = {"data": None, "fetched_at": 0}
_CONCEPT_CACHE_TTL = 60 * 60  # 1 hour TTL
# SDTM dataset specializations cache (similar TTL)
_sdtm_specializations_cache = {"data": None, "fetched_at": 0}
_SDTM_SPECIALIZATIONS_CACHE_TTL = 60 * 60
app = FastAPI(title="SoA Builder API", version="0.1.0")
logger = logging.getLogger("soa_builder.concepts")
if not logger.handlers:
    _h = logging.StreamHandler()
    _h.setFormatter(logging.Formatter("[%(asctime)s] %(levelname)s %(message)s"))
    logger.addHandler(_h)
logger.setLevel(logging.INFO)
TEMPLATES_DIR = os.path.join(os.path.dirname(__file__), "templates")
STATIC_DIR = os.path.join(os.path.dirname(__file__), "static")
os.makedirs(TEMPLATES_DIR, exist_ok=True)
os.makedirs(STATIC_DIR, exist_ok=True)
templates = Jinja2Templates(directory=TEMPLATES_DIR)
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# --------------------- DB bootstrap ---------------------


def _connect():
    return sqlite3.connect(DB_PATH)


def _init_db():
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        """CREATE TABLE IF NOT EXISTS soa (id INTEGER PRIMARY KEY AUTOINCREMENT, name TEXT, created_at TEXT)"""
    )
    cur.execute(
        """CREATE TABLE IF NOT EXISTS visit (id INTEGER PRIMARY KEY AUTOINCREMENT, soa_id INTEGER, name TEXT, raw_header TEXT, order_index INTEGER)"""
    )
    cur.execute(
        """CREATE TABLE IF NOT EXISTS activity (id INTEGER PRIMARY KEY AUTOINCREMENT, soa_id INTEGER, name TEXT, order_index INTEGER, activity_uid TEXT)"""
    )
    # Arms: groupings similar to Visits. (Legacy element linkage removed; schema now only stores intrinsic fields.)
    cur.execute(
        """CREATE TABLE IF NOT EXISTS arm (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            name TEXT NOT NULL,
            label TEXT,
            description TEXT,
            order_index INTEGER,
            arm_uid TEXT -- immutable StudyArm_N identifier unique within an SOA
        )"""
    )
    # Elements: finer-grained structural units (optional) that can also be ordered
    cur.execute(
        """CREATE TABLE IF NOT EXISTS element (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            name TEXT NOT NULL,
            label TEXT,
            description TEXT,
            testrl TEXT,
            teenrl TEXT,
            order_index INTEGER,
            created_at TEXT
        )"""
    )
    # Element audit table capturing create/update/delete operations
    cur.execute(
        """CREATE TABLE IF NOT EXISTS element_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            element_id INTEGER,
            action TEXT NOT NULL, -- create|update|delete|reorder
            before_json TEXT,
            after_json TEXT,
            performed_at TEXT NOT NULL
        )"""
    )
    # Visit audit table
    cur.execute(
        """CREATE TABLE IF NOT EXISTS visit_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            visit_id INTEGER,
            action TEXT NOT NULL, -- create|update|delete|reorder
            before_json TEXT,
            after_json TEXT,
            performed_at TEXT NOT NULL
        )"""
    )
    # Activity audit table
    cur.execute(
        """CREATE TABLE IF NOT EXISTS activity_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            activity_id INTEGER,
            action TEXT NOT NULL, -- create|update|delete|reorder
            before_json TEXT,
            after_json TEXT,
            performed_at TEXT NOT NULL
        )"""
    )
    # Arm audit table
    cur.execute(
        """CREATE TABLE IF NOT EXISTS arm_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            arm_id INTEGER,
            action TEXT NOT NULL, -- create|update|delete|reorder
            before_json TEXT,
            after_json TEXT,
            performed_at TEXT NOT NULL
        )"""
    )
    # Epoch audit table
    cur.execute(
        """CREATE TABLE IF NOT EXISTS epoch_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            epoch_id INTEGER,
            action TEXT NOT NULL, -- create|update|delete|reorder
            before_json TEXT,
            after_json TEXT,
            performed_at TEXT NOT NULL
        )"""
    )
    # Epochs: high-level study phase grouping (optional). Behaves like visits/activities list ordering.
    cur.execute(
        """CREATE TABLE IF NOT EXISTS epoch (id INTEGER PRIMARY KEY AUTOINCREMENT, soa_id INTEGER, name TEXT, order_index INTEGER)"""
    )
    cur.execute(
        """CREATE TABLE IF NOT EXISTS cell (id INTEGER PRIMARY KEY AUTOINCREMENT, soa_id INTEGER, visit_id INTEGER, activity_id INTEGER, status TEXT)"""
    )
    # Mapping table linking activities to biomedical concepts (concept_code + title stored for snapshot purposes)
    cur.execute(
        """CREATE TABLE IF NOT EXISTS activity_concept (id INTEGER PRIMARY KEY AUTOINCREMENT, activity_id INTEGER, concept_code TEXT, concept_title TEXT)"""
    )
    # Frozen versions (snapshot JSON of current matrix & concepts)
    cur.execute(
        """CREATE TABLE IF NOT EXISTS soa_freeze (id INTEGER PRIMARY KEY AUTOINCREMENT, soa_id INTEGER, version_label TEXT, created_at TEXT, snapshot_json TEXT)"""
    )
    # Unique index to enforce one label per SoA
    cur.execute(
        """CREATE UNIQUE INDEX IF NOT EXISTS idx_soafreeze_unique ON soa_freeze(soa_id, version_label)"""
    )
    # Rollback audit log
    cur.execute(
        """CREATE TABLE IF NOT EXISTS rollback_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            freeze_id INTEGER NOT NULL,
            performed_at TEXT NOT NULL,
            visits_restored INTEGER,
            activities_restored INTEGER,
            cells_restored INTEGER,
            concepts_restored INTEGER,
            elements_restored INTEGER
        )"""
    )
    # Reorder audit (tracks manual drag reorder operations for visits & activities)
    cur.execute(
        """CREATE TABLE IF NOT EXISTS reorder_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            soa_id INTEGER NOT NULL,
            entity_type TEXT NOT NULL, -- 'visit' | 'activity' | 'epoch' | 'arm' | 'element'
            old_order_json TEXT NOT NULL,
            new_order_json TEXT NOT NULL,
            performed_at TEXT NOT NULL
        )"""
    )
    conn.commit()
    conn.close()


_init_db()


# --------------------- Migration: add arm_uid to arm ---------------------
def _migrate_add_arm_uid():
    """Ensure arm_uid column exists and is populated with StudyArm_<n> unique per soa.
    Backfills existing arms sequentially by id order if missing. Creates unique index (soa_id, arm_uid).
    """
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(arm)")
        cols = {r[1] for r in cur.fetchall()}
        if "arm_uid" not in cols:
            cur.execute("ALTER TABLE arm ADD COLUMN arm_uid TEXT")
            conn.commit()
        # Backfill any NULL arm_uid values
        cur.execute("SELECT DISTINCT soa_id FROM arm WHERE arm_uid IS NULL")
        soa_ids = [r[0] for r in cur.fetchall()]
        for sid in soa_ids:
            cur.execute(
                "SELECT id FROM arm WHERE soa_id=? AND arm_uid IS NULL ORDER BY id",
                (sid,),
            )
            ids = [r[0] for r in cur.fetchall()]
            # Determine existing numbers to avoid collision (if partial data present)
            cur.execute(
                "SELECT arm_uid FROM arm WHERE soa_id=? AND arm_uid IS NOT NULL", (sid,)
            )
            existing_uids = {r[0] for r in cur.fetchall() if r[0]}
            used_nums = set()
            for uid in existing_uids:
                if uid.startswith("StudyArm_"):
                    try:
                        used_nums.add(int(uid.split("StudyArm_")[-1]))
                    except Exception:
                        pass
            next_n = 1
            for arm_id in ids:
                while next_n in used_nums:
                    next_n += 1
                new_uid = f"StudyArm_{next_n}"
                used_nums.add(next_n)
                next_n += 1
                cur.execute("UPDATE arm SET arm_uid=? WHERE id=?", (new_uid, arm_id))
        # Create unique index
        try:
            cur.execute(
                "CREATE UNIQUE INDEX IF NOT EXISTS idx_arm_soaid_uid ON arm(soa_id, arm_uid)"
            )
            conn.commit()
        except Exception:
            pass
        conn.commit()
        conn.close()
    except Exception as e:
        logger.warning("arm_uid migration failed: %s", e)


_migrate_add_arm_uid()


# --------------------- Migration: drop deprecated arm linkage columns ---------------------
def _migrate_drop_arm_element_link():
    """If legacy columns (element_id, etcd) exist in arm, rebuild table without them.
    SQLite cannot drop columns directly; we create new table, copy data, replace.
    Safe to run multiple times (idempotent)."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(arm)")
        cols = [r[1] for r in cur.fetchall()]
        if "element_id" in cols or "etcd" in cols:
            logger.info(
                "Rebuilding arm table to drop deprecated columns element_id, etcd"
            )
            # Determine if arm_uid index exists to recreate later
            cur.execute(
                "SELECT name FROM sqlite_master WHERE type='index' AND name='idx_arm_soaid_uid'"
            )
            has_uid_index = cur.fetchone() is not None
            # Create new table
            cur.execute(
                """
                CREATE TABLE arm_new (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    soa_id INTEGER NOT NULL,
                    name TEXT NOT NULL,
                    label TEXT,
                    description TEXT,
                    order_index INTEGER,
                    arm_uid TEXT
                )
            """
            )
            # Copy data (ignore legacy columns)
            # Only select columns that persist
            select_cols = [
                c
                for c in [
                    "id",
                    "soa_id",
                    "name",
                    "label",
                    "description",
                    "order_index",
                    "arm_uid",
                ]
                if c in cols
            ]
            cur.execute(
                f"INSERT INTO arm_new (id,soa_id,name,label,description,order_index,arm_uid) SELECT id,soa_id,name,label,description,order_index,arm_uid FROM arm"
            )
            # Drop old table, rename
            cur.execute("DROP TABLE arm")
            cur.execute("ALTER TABLE arm_new RENAME TO arm")
            if has_uid_index:
                try:
                    cur.execute(
                        "CREATE UNIQUE INDEX IF NOT EXISTS idx_arm_soaid_uid ON arm(soa_id, arm_uid)"
                    )
                except Exception:
                    pass
            conn.commit()
        conn.close()
    except Exception as e:
        logger.warning("arm linkage drop migration failed: %s", e)


_migrate_drop_arm_element_link()


# --------------------- Migration: add epoch_id to visit ---------------------
def _migrate_add_epoch_id_to_visit():
    """Add epoch_id column to visit table if missing."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(visit)")
        cols = {r[1] for r in cur.fetchall()}
        if "epoch_id" not in cols:
            cur.execute("ALTER TABLE visit ADD COLUMN epoch_id INTEGER")
            conn.commit()
            logger.info("Added epoch_id column to visit table")
        conn.close()
    except Exception as e:
        logger.warning("epoch_id migration failed: %s", e)


_migrate_add_epoch_id_to_visit()


# --------------------- Migration: add epoch_seq to epoch ---------------------
def _migrate_add_epoch_seq():
    """Ensure epoch_seq (immutable sequence per SoA) exists; backfill sequential values per study.
    Creates unique index (soa_id, epoch_seq) to guarantee uniqueness inside a study.
    """
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(epoch)")
        cols = {r[1] for r in cur.fetchall()}
        if "epoch_seq" not in cols:
            cur.execute("ALTER TABLE epoch ADD COLUMN epoch_seq INTEGER")
            conn.commit()
            logger.info("Added epoch_seq column to epoch table")
            # Backfill existing epochs with sequential values by id order per soa
            cur.execute("SELECT DISTINCT soa_id FROM epoch")
            soa_ids = [r[0] for r in cur.fetchall()]
            for sid in soa_ids:
                cur.execute("SELECT id FROM epoch WHERE soa_id=? ORDER BY id", (sid,))
                ids = [r[0] for r in cur.fetchall()]
                for seq, eid in enumerate(ids, start=1):
                    cur.execute("UPDATE epoch SET epoch_seq=? WHERE id=?", (seq, eid))
            conn.commit()
        # Unique index (idempotent)
        try:
            cur.execute(
                "CREATE UNIQUE INDEX IF NOT EXISTS idx_epoch_soaid_seq ON epoch(soa_id, epoch_seq)"
            )
            conn.commit()
        except Exception as ie:  # pragma: no cover
            logger.warning("Failed creating idx_epoch_soaid_seq: %s", ie)
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("epoch_seq migration failed: %s", e)


_migrate_add_epoch_seq()


# --------------------- Migration: add epoch label/description ---------------------
def _migrate_add_epoch_label_desc():
    """Add optional epoch_label and epoch_description columns if missing."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(epoch)")
        cols = {r[1] for r in cur.fetchall()}
        alters = []
        if "epoch_label" not in cols:
            alters.append("ALTER TABLE epoch ADD COLUMN epoch_label TEXT")
        if "epoch_description" not in cols:
            alters.append("ALTER TABLE epoch ADD COLUMN epoch_description TEXT")
        for stmt in alters:
            try:
                cur.execute(stmt)
            except Exception as e:  # pragma: no cover
                logger.warning(
                    "Failed epoch label/description migration '%s': %s", stmt, e
                )
        if alters:
            conn.commit()
            logger.info(
                "Applied epoch label/description migration: %s", ", ".join(alters)
            )
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("Epoch label/description migration failed: %s", e)


_migrate_add_epoch_label_desc()
# --------------------- Migrations: add study metadata columns ---------------------


def _migrate_add_study_fields():
    """Ensure study metadata columns (study_id, study_label, study_description) exist on soa table.
    Safe to run repeatedly; SQLite ADD COLUMN is idempotent when guarded by schema inspection.
    """
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(soa)")
        existing = {r[1] for r in cur.fetchall()}  # column names
        alters = []
        if "study_id" not in existing:
            alters.append("ALTER TABLE soa ADD COLUMN study_id TEXT")
        if "study_label" not in existing:
            alters.append("ALTER TABLE soa ADD COLUMN study_label TEXT")
        if "study_description" not in existing:
            alters.append("ALTER TABLE soa ADD COLUMN study_description TEXT")
        for stmt in alters:
            try:
                cur.execute(stmt)
            except (
                Exception
            ) as e:  # pragma: no cover - defensive; should not fail normally
                logger.warning("Failed executing migration statement '%s': %s", stmt, e)
        if alters:
            conn.commit()
        # Create unique index on study_id (NULLs allowed multiple times by SQLite)
        try:
            cur.execute(
                "CREATE UNIQUE INDEX IF NOT EXISTS idx_soa_study_id ON soa(study_id)"
            )
            conn.commit()
        except Exception as e:  # pragma: no cover
            logger.warning("Failed creating unique index idx_soa_study_id: %s", e)
        conn.close()
        if alters:
            logger.info("Applied study field migrations: %s", ", ".join(alters))
    except Exception as e:  # pragma: no cover
        logger.warning("Study field migration failed: %s", e)


_migrate_add_study_fields()

# --------------------- Migrations ---------------------


def _drop_unused_override_table():
    """Drop legacy activity_concept_override table if it still exists.
    This table supported mutable concept titles which are no longer allowed.
    Safe to run repeatedly; will no-op if table absent."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='activity_concept_override'"
        )
        if cur.fetchone():
            try:
                cur.execute("DROP TABLE activity_concept_override")
                conn.commit()
                logger.info("Dropped obsolete table activity_concept_override")
            except Exception as e:
                logger.warning(
                    "Failed to drop obsolete table activity_concept_override: %s", e
                )
        conn.close()
    except Exception as e:
        logger.warning("Migration check for activity_concept_override failed: %s", e)


_drop_unused_override_table()


# --------------------- Migration: ensure element table columns ---------------------
def _migrate_element_table():
    """Ensure element table has full expected schema (order_index, label, description, testrl, teenrl, created_at).
    Backfills order_index sequentially by id if missing.
    Safe to run repeatedly."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name='element'"
        )
        if not cur.fetchone():
            conn.close()
            return  # table does not exist yet (fresh init will create with full schema)
        cur.execute("PRAGMA table_info(element)")
        cols = {r[1] for r in cur.fetchall()}
        alters = []
        # Add missing columns
        if "order_index" not in cols:
            alters.append("ALTER TABLE element ADD COLUMN order_index INTEGER")
        if "label" not in cols:
            alters.append("ALTER TABLE element ADD COLUMN label TEXT")
        if "description" not in cols:
            alters.append("ALTER TABLE element ADD COLUMN description TEXT")
        if "testrl" not in cols:
            alters.append("ALTER TABLE element ADD COLUMN testrl TEXT")
        if "teenrl" not in cols:
            alters.append("ALTER TABLE element ADD COLUMN teenrl TEXT")
        if "created_at" not in cols:
            alters.append("ALTER TABLE element ADD COLUMN created_at TEXT")
        for stmt in alters:
            try:
                cur.execute(stmt)
            except Exception as e:  # pragma: no cover
                logger.warning("Element migration failed executing '%s': %s", stmt, e)
        if alters:
            conn.commit()
        # Backfill order_index if newly added
        if "order_index" not in cols:
            cur.execute("SELECT id FROM element ORDER BY id")
            ids = [r[0] for r in cur.fetchall()]
            for idx, eid in enumerate(ids, start=1):
                cur.execute("UPDATE element SET order_index=? WHERE id=?", (idx, eid))
        # Backfill created_at
        if "created_at" not in cols:
            now = datetime.utcnow().isoformat()
            cur.execute(
                "UPDATE element SET created_at=? WHERE created_at IS NULL", (now,)
            )
        conn.commit()
        conn.close()
        if alters:
            logger.info("Applied element table migration: %s", ", ".join(alters))
    except Exception as e:  # pragma: no cover
        logger.warning("Element table migration encountered error: %s", e)


_migrate_element_table()


# --------------------- Migration: add elements_restored to rollback_audit ---------------------
def _migrate_rollback_add_elements_restored():
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(rollback_audit)")
        cols = {r[1] for r in cur.fetchall()}
        if "elements_restored" not in cols:
            cur.execute(
                "ALTER TABLE rollback_audit ADD COLUMN elements_restored INTEGER"
            )
            conn.commit()
            logger.info("Added elements_restored column to rollback_audit")
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("rollback_audit migration failed: %s", e)


_migrate_rollback_add_elements_restored()


# --------------------- Migration: add activity_uid to activity ---------------------
def _migrate_activity_add_uid():
    """Add activity_uid column if missing; backfill as Activity_<order_index>."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute("PRAGMA table_info(activity)")
        cols = {r[1] for r in cur.fetchall()}
        if "activity_uid" not in cols:
            cur.execute("ALTER TABLE activity ADD COLUMN activity_uid TEXT")
            # backfill
            cur.execute("SELECT id, order_index FROM activity")
            for rid, oi in cur.fetchall():
                cur.execute(
                    "UPDATE activity SET activity_uid=? WHERE id=?",
                    (f"Activity_{oi}", rid),
                )
            # create unique index scoped per soa
            cur.execute(
                "CREATE UNIQUE INDEX IF NOT EXISTS idx_activity_soa_uid ON activity(soa_id, activity_uid)"
            )
            conn.commit()
        else:
            # still ensure index exists
            cur.execute(
                "CREATE UNIQUE INDEX IF NOT EXISTS idx_activity_soa_uid ON activity(soa_id, activity_uid)"
            )
            conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("activity_uid migration failed: %s", e)


_migrate_activity_add_uid()


# --------------------- Backfill dataset_date for existing terminology tables ---------------------
def _backfill_dataset_date(table: str, audit_table: str):
    """If terminology table exists and has dataset_date (or sheet_dataset_date) column with blank values,
    attempt to backfill from the latest audit row that has a non-null dataset_date.
    Safe to run multiple times; will no-op if already populated or columns absent."""
    try:
        conn = _connect()
        cur = conn.cursor()
        # Ensure table exists
        cur.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name=?", (table,)
        )
        if not cur.fetchone():
            conn.close()
            return
        cur.execute(f"PRAGMA table_info({table})")
        cols = {r[1] for r in cur.fetchall()}
        date_col = None
        # Prefer dataset_date; fallback sheet_dataset_date
        if "dataset_date" in cols:
            date_col = "dataset_date"
        elif "sheet_dataset_date" in cols:
            date_col = "sheet_dataset_date"
        if not date_col:
            conn.close()
            return
        # Check if any non-empty value exists
        cur.execute(
            f"SELECT COUNT(*) FROM {table} WHERE {date_col} IS NOT NULL AND {date_col} != ''"
        )
        if cur.fetchone()[0] > 0:
            conn.close()
            return  # already populated
        # Find latest audit dataset_date
        cur.execute(
            f"SELECT dataset_date FROM {audit_table} WHERE dataset_date IS NOT NULL AND dataset_date != '' ORDER BY loaded_at DESC LIMIT 1"
        )
        row = cur.fetchone()
        if not row or not row[0]:
            conn.close()
            return
        ds_date = row[0]
        cur.execute(
            f"UPDATE {table} SET {date_col}=? WHERE {date_col} IS NULL OR {date_col}=''",
            (ds_date,),
        )
        conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("dataset_date backfill for %s failed: %s", table, e)


_backfill_dataset_date("ddf_terminology", "ddf_terminology_audit")
_backfill_dataset_date("protocol_terminology", "protocol_terminology_audit")

# --------------------- Models ---------------------


class SOACreate(BaseModel):
    name: str
    study_id: Optional[str] = None
    study_label: Optional[str] = None
    study_description: Optional[str] = None


class SOAMetadataUpdate(BaseModel):
    study_id: Optional[str] = None
    study_label: Optional[str] = None
    study_description: Optional[str] = None


class VisitCreate(BaseModel):
    name: str
    raw_header: Optional[str] = None
    epoch_id: Optional[int] = None


class ActivityCreate(BaseModel):
    name: str


class ElementCreate(BaseModel):
    name: str
    label: Optional[str] = None
    description: Optional[str] = None
    testrl: Optional[str] = None  # start rule (optional)
    teenrl: Optional[str] = None  # end rule (optional)


class ElementUpdate(BaseModel):
    name: Optional[str] = None
    label: Optional[str] = None
    description: Optional[str] = None
    testrl: Optional[str] = None
    teenrl: Optional[str] = None


class VisitUpdate(BaseModel):
    name: Optional[str] = None
    raw_header: Optional[str] = None
    epoch_id: Optional[int] = None


class ActivityUpdate(BaseModel):
    name: Optional[str] = None


class ArmCreate(BaseModel):
    name: str
    label: Optional[str] = None
    description: Optional[str] = None
    # element linkage removed; arms are now independent of elements.


class ArmUpdate(BaseModel):
    name: Optional[str] = None
    label: Optional[str] = None
    description: Optional[str] = None
    # element linkage removed


def _record_element_audit(
    soa_id: int,
    action: str,
    element_id: Optional[int],
    before: Optional[dict] = None,
    after: Optional[dict] = None,
):
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO element_audit (soa_id, element_id, action, before_json, after_json, performed_at) VALUES (?,?,?,?,?,?)",
            (
                soa_id,
                element_id,
                action,
                json.dumps(before) if before else None,
                json.dumps(after) if after else None,
                datetime.now(timezone.utc).isoformat(),
            ),
        )
        conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("Failed recording element audit: %s", e)


def _record_visit_audit(
    soa_id: int,
    action: str,
    visit_id: Optional[int],
    before: Optional[dict] = None,
    after: Optional[dict] = None,
):
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO visit_audit (soa_id, visit_id, action, before_json, after_json, performed_at) VALUES (?,?,?,?,?,?)",
            (
                soa_id,
                visit_id,
                action,
                json.dumps(before) if before else None,
                json.dumps(after) if after else None,
                datetime.now(timezone.utc).isoformat(),
            ),
        )
        conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("Failed recording visit audit: %s", e)


def _record_activity_audit(
    soa_id: int,
    action: str,
    activity_id: Optional[int],
    before: Optional[dict] = None,
    after: Optional[dict] = None,
):
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO activity_audit (soa_id, activity_id, action, before_json, after_json, performed_at) VALUES (?,?,?,?,?,?)",
            (
                soa_id,
                activity_id,
                action,
                json.dumps(before) if before else None,
                json.dumps(after) if after else None,
                datetime.now(timezone.utc).isoformat(),
            ),
        )
        conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("Failed recording activity audit: %s", e)


def _record_epoch_audit(
    soa_id: int,
    action: str,
    epoch_id: Optional[int],
    before: Optional[dict] = None,
    after: Optional[dict] = None,
):
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO epoch_audit (soa_id, epoch_id, action, before_json, after_json, performed_at) VALUES (?,?,?,?,?,?)",
            (
                soa_id,
                epoch_id,
                action,
                json.dumps(before) if before else None,
                json.dumps(after) if after else None,
                datetime.now(timezone.utc).isoformat(),
            ),
        )
        conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("Failed recording epoch audit: %s", e)


def _record_arm_audit(
    soa_id: int,
    action: str,
    arm_id: Optional[int],
    before: Optional[dict] = None,
    after: Optional[dict] = None,
):
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO arm_audit (soa_id, arm_id, action, before_json, after_json, performed_at) VALUES (?,?,?,?,?,?)",
            (
                soa_id,
                arm_id,
                action,
                json.dumps(before) if before else None,
                json.dumps(after) if after else None,
                datetime.now(timezone.utc).isoformat(),
            ),
        )
        conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("Failed recording arm audit: %s", e)


# --------------------- Element REST Endpoints ---------------------
@app.get("/soa/{soa_id}/elements", response_class=JSONResponse)
def list_elements(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    rows = [
        {
            "id": r[0],
            "name": r[1],
            "label": r[2],
            "description": r[3],
            "testrl": r[4],
            "teenrl": r[5],
            "order_index": r[6],
            "created_at": r[7],
        }
        for r in cur.fetchall()
    ]
    conn.close()
    return JSONResponse(rows)


@app.get("/soa/{soa_id}/elements/{element_id}", response_class=JSONResponse)
def get_element(soa_id: int, element_id: int):
    """Return details for a single element (parity with visit/activity/epoch detail endpoints)."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE id=? AND soa_id=?",
        (element_id, soa_id),
    )
    r = cur.fetchone()
    conn.close()
    if not r:
        raise HTTPException(404, "Element not found")
    return {
        "id": r[0],
        "soa_id": soa_id,
        "name": r[1],
        "label": r[2],
        "description": r[3],
        "testrl": r[4],
        "teenrl": r[5],
        "order_index": r[6],
        "created_at": r[7],
    }


@app.get("/soa/{soa_id}/element_audit", response_class=JSONResponse)
def list_element_audit(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id, element_id, action, before_json, after_json, performed_at FROM element_audit WHERE soa_id=? ORDER BY id DESC",
        (soa_id,),
    )
    rows = []
    for r in cur.fetchall():
        try:
            before = json.loads(r[3]) if r[3] else None
        except Exception:
            before = None
        try:
            after = json.loads(r[4]) if r[4] else None
        except Exception:
            after = None
        rows.append(
            {
                "id": r[0],
                "element_id": r[1],
                "action": r[2],
                "before": before,
                "after": after,
                "performed_at": r[5],
            }
        )
    conn.close()
    return JSONResponse(rows)


@app.post("/soa/{soa_id}/elements", response_class=JSONResponse, status_code=201)
def create_element(soa_id: int, payload: ElementCreate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    name = (payload.name or "").strip()
    if not name:
        raise HTTPException(400, "Name required")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT COALESCE(MAX(order_index),0) FROM element WHERE soa_id=?", (soa_id,)
    )
    next_ord = (cur.fetchone() or [0])[0] + 1
    now = datetime.utcnow().isoformat()
    cur.execute(
        """INSERT INTO element (soa_id,name,label,description,testrl,teenrl,order_index,created_at)
        VALUES (?,?,?,?,?,?,?,?)""",
        (
            soa_id,
            name,
            (payload.label or "").strip() or None,
            (payload.description or "").strip() or None,
            (payload.testrl or "").strip() or None,
            (payload.teenrl or "").strip() or None,
            next_ord,
            now,
        ),
    )
    eid = cur.lastrowid
    conn.commit()
    conn.close()
    el = {
        "id": eid,
        "name": name,
        "label": (payload.label or "").strip() or None,
        "description": (payload.description or "").strip() or None,
        "testrl": (payload.testrl or "").strip() or None,
        "teenrl": (payload.teenrl or "").strip() or None,
        "order_index": next_ord,
        "created_at": now,
    }
    _record_element_audit(soa_id, "create", eid, before=None, after=el)
    # FastAPI will apply the declared status_code=201 automatically.
    return el


@app.patch("/soa/{soa_id}/elements/{element_id}", response_class=JSONResponse)
def update_element(soa_id: int, element_id: int, payload: ElementUpdate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE id=? AND soa_id=?",
        (element_id, soa_id),
    )
    row = cur.fetchone()
    if not row:
        conn.close()
        raise HTTPException(404, "Element not found")
    before = {
        "id": row[0],
        "name": row[1],
        "label": row[2],
        "description": row[3],
        "testrl": row[4],
        "teenrl": row[5],
        "order_index": row[6],
        "created_at": row[7],
    }
    new_name = (payload.name if payload.name is not None else before["name"]) or ""
    cur.execute(
        "UPDATE element SET name=?, label=?, description=?, testrl=?, teenrl=? WHERE id=?",
        (
            (new_name or "").strip() or None,
            (payload.label if payload.label is not None else before["label"]),
            (
                payload.description
                if payload.description is not None
                else before["description"]
            ),
            (payload.testrl if payload.testrl is not None else before["testrl"]),
            (payload.teenrl if payload.teenrl is not None else before["teenrl"]),
            element_id,
        ),
    )
    conn.commit()
    # Fetch updated
    cur.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE id=?",
        (element_id,),
    )
    r = cur.fetchone()
    conn.close()
    after = {
        "id": r[0],
        "name": r[1],
        "label": r[2],
        "description": r[3],
        "testrl": r[4],
        "teenrl": r[5],
        "order_index": r[6],
        "created_at": r[7],
    }
    # Determine which mutable fields actually changed (excluding id, order_index, created_at)
    mutable_fields = ["name", "label", "description", "testrl", "teenrl"]
    updated_fields = [f for f in mutable_fields if before.get(f) != after.get(f)]
    _record_element_audit(
        soa_id,
        "update",
        element_id,
        before=before,
        after={**after, "updated_fields": updated_fields},
    )
    return JSONResponse({**after, "updated_fields": updated_fields})


@app.delete("/soa/{soa_id}/elements/{element_id}", response_class=JSONResponse)
def delete_element(soa_id: int, element_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE id=? AND soa_id=?",
        (element_id, soa_id),
    )
    row = cur.fetchone()
    if not row:
        conn.close()
        raise HTTPException(404, "Element not found")
    before = {
        "id": row[0],
        "name": row[1],
        "label": row[2],
        "description": row[3],
        "testrl": row[4],
        "teenrl": row[5],
        "order_index": row[6],
        "created_at": row[7],
    }
    cur.execute("DELETE FROM element WHERE id=?", (element_id,))
    conn.commit()
    conn.close()
    _record_element_audit(soa_id, "delete", element_id, before=before, after=None)
    return JSONResponse({"deleted": True, "id": element_id})


@app.post("/soa/{soa_id}/elements/reorder", response_class=JSONResponse)
def reorder_elements_api(soa_id: int, order: List[int]):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    if not order:
        raise HTTPException(400, "Order list required")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM element WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM element WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(order) - existing:
        conn.close()
        raise HTTPException(400, "Order contains invalid element id")
    for idx, eid in enumerate(order, start=1):
        cur.execute("UPDATE element SET order_index=? WHERE id=?", (idx, eid))
    conn.commit()
    conn.close()
    _record_element_audit(
        soa_id,
        "reorder",
        element_id=None,
        before={"old_order": old_order},
        after={"new_order": order},
    )
    return JSONResponse({"ok": True, "old_order": old_order, "new_order": order})


# --------------------- Arm REST Endpoints ---------------------
@app.get("/soa/{soa_id}/arms", response_class=JSONResponse)
def list_arms(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,label,description,order_index,arm_uid FROM arm WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    rows = [
        {
            "id": r[0],
            "name": r[1],
            "label": r[2],
            "description": r[3],
            "order_index": r[4],
            "arm_uid": r[5],
        }
        for r in cur.fetchall()
    ]
    conn.close()
    return rows


@app.get("/soa/{soa_id}/arm_audit", response_class=JSONResponse)
def list_arm_audit(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id, arm_id, action, before_json, after_json, performed_at FROM arm_audit WHERE soa_id=? ORDER BY id DESC",
        (soa_id,),
    )
    rows = []
    for r in cur.fetchall():
        try:
            before = json.loads(r[3]) if r[3] else None
        except Exception:
            before = None
        try:
            after = json.loads(r[4]) if r[4] else None
        except Exception:
            after = None
        rows.append(
            {
                "id": r[0],
                "arm_id": r[1],
                "action": r[2],
                "before": before,
                "after": after,
                "performed_at": r[5],
            }
        )
    conn.close()
    return rows


@app.post("/soa/{soa_id}/arms", response_class=JSONResponse, status_code=201)
def create_arm(soa_id: int, payload: ArmCreate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    name = (payload.name or "").strip()
    if not name:
        raise HTTPException(400, "Name required")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT COALESCE(MAX(order_index),0) FROM arm WHERE soa_id=?", (soa_id,)
    )
    next_ord = (cur.fetchone() or [0])[0] + 1
    # Generate next arm_uid (StudyArm_N) unique within this SoA
    cur.execute(
        "SELECT arm_uid FROM arm WHERE soa_id=? AND arm_uid LIKE 'StudyArm_%'",
        (soa_id,),
    )
    existing_uids = [r[0] for r in cur.fetchall() if r[0]]
    used_nums = set()
    for uid in existing_uids:
        try:
            used_nums.add(int(uid.split("StudyArm_")[-1]))
        except Exception:
            pass
    next_n = 1
    while next_n in used_nums:
        next_n += 1
    new_uid = f"StudyArm_{next_n}"
    # element linkage removed: etcd always NULL
    etcd_val = None
    cur.execute(
        """INSERT INTO arm (soa_id,name,label,description,order_index,arm_uid)
            VALUES (?,?,?,?,?,?)""",
        (
            soa_id,
            name,
            (payload.label or "").strip() or None,
            (payload.description or "").strip() or None,
            next_ord,
            new_uid,
        ),
    )
    arm_id = cur.lastrowid
    conn.commit()
    conn.close()
    row = {
        "id": arm_id,
        "name": name,
        "label": (payload.label or "").strip() or None,
        "description": (payload.description or "").strip() or None,
        "order_index": next_ord,
        "arm_uid": new_uid,
    }
    _record_arm_audit(soa_id, "create", arm_id, before=None, after=row)
    return row


@app.patch("/soa/{soa_id}/arms/{arm_id}", response_class=JSONResponse)
def update_arm(soa_id: int, arm_id: int, payload: ArmUpdate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,label,description,order_index,arm_uid FROM arm WHERE id=? AND soa_id=?",
        (arm_id, soa_id),
    )
    row = cur.fetchone()
    if not row:
        conn.close()
        raise HTTPException(404, "Arm not found")
    before = {
        "id": row[0],
        "name": row[1],
        "label": row[2],
        "description": row[3],
        "order_index": row[6],
        "arm_uid": row[7],
    }
    new_name = (payload.name if payload.name is not None else before["name"]) or ""
    new_label = payload.label if payload.label is not None else before["label"]
    new_desc = (
        payload.description
        if payload.description is not None
        else before["description"]
    )
    cur.execute(
        "UPDATE arm SET name=?, label=?, description=? WHERE id=?",
        (
            (new_name or "").strip() or None,
            (new_label or "").strip() or None,
            (new_desc or "").strip() or None,
            arm_id,
        ),
    )
    conn.commit()
    cur.execute(
        "SELECT id,name,label,description,order_index,arm_uid FROM arm WHERE id=?",
        (arm_id,),
    )
    r = cur.fetchone()
    conn.close()
    after = {
        "id": r[0],
        "name": r[1],
        "label": r[2],
        "description": r[3],
        "order_index": r[4],
        "arm_uid": r[5],
    }
    mutable = ["name", "label", "description"]  # arm_uid immutable; linkage removed
    updated_fields = [f for f in mutable if before.get(f) != after.get(f)]
    _record_arm_audit(
        soa_id,
        "update",
        arm_id,
        before=before,
        after={**after, "updated_fields": updated_fields},
    )
    return {**after, "updated_fields": updated_fields}


@app.delete("/soa/{soa_id}/arms/{arm_id}", response_class=JSONResponse)
def delete_arm(soa_id: int, arm_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,label,description,order_index,arm_uid FROM arm WHERE id=? AND soa_id=?",
        (arm_id, soa_id),
    )
    row = cur.fetchone()
    if not row:
        conn.close()
        raise HTTPException(404, "Arm not found")
    before = {
        "id": row[0],
        "name": row[1],
        "label": row[2],
        "description": row[3],
        "order_index": row[4],
        "arm_uid": row[5],
    }
    cur.execute("DELETE FROM arm WHERE id=?", (arm_id,))
    conn.commit()
    conn.close()
    _record_arm_audit(soa_id, "delete", arm_id, before=before, after=None)
    return {"deleted": True, "id": arm_id}


@app.post("/soa/{soa_id}/arms/reorder", response_class=JSONResponse)
def reorder_arms_api(soa_id: int, order: List[int]):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    if not order:
        raise HTTPException(400, "Order list required")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM arm WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM arm WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(order) - existing:
        conn.close()
        raise HTTPException(400, "Order contains invalid arm id")
    for idx, aid in enumerate(order, start=1):
        cur.execute("UPDATE arm SET order_index=? WHERE id=?", (idx, aid))
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "arm", old_order, order)
    _record_arm_audit(
        soa_id,
        "reorder",
        arm_id=None,
        before={"old_order": old_order},
        after={"new_order": order},
    )
    return {"ok": True, "old_order": old_order, "new_order": order}


@app.post("/soa/{soa_id}/visits/reorder", response_class=JSONResponse)
def reorder_visits_api(soa_id: int, order: List[int]):
    """JSON reorder endpoint for visits (parity with elements). Body is array of visit IDs in desired order."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    if not order:
        raise HTTPException(400, "Order list required")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM visit WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM visit WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(order) - existing:
        conn.close()
        raise HTTPException(400, "Order contains invalid visit id")
    for idx, vid in enumerate(order, start=1):
        cur.execute("UPDATE visit SET order_index=? WHERE id=?", (idx, vid))
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "visit", old_order, order)
    return JSONResponse({"ok": True, "old_order": old_order, "new_order": order})


@app.post("/soa/{soa_id}/activities/reorder", response_class=JSONResponse)
def reorder_activities_api(soa_id: int, order: List[int]):
    """JSON reorder endpoint for activities."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    if not order:
        raise HTTPException(400, "Order list required")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id FROM activity WHERE soa_id=? ORDER BY order_index", (soa_id,)
    )
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM activity WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(order) - existing:
        conn.close()
        raise HTTPException(400, "Order contains invalid activity id")
    # Capture before state for audit detail (id -> order_index)
    before_rows = {
        r[0]: r[1]
        for r in cur.execute(
            "SELECT id, order_index FROM activity WHERE soa_id=?", (soa_id,)
        ).fetchall()
    }
    for idx, aid in enumerate(order, start=1):
        cur.execute("UPDATE activity SET order_index=? WHERE id=?", (idx, aid))
    # Prepare after state mapping prior to UID refresh
    after_rows = {
        r[0]: r[1]
        for r in cur.execute(
            "SELECT id, order_index FROM activity WHERE soa_id=?", (soa_id,)
        ).fetchall()
    }
    # Two-phase UID reassignment to avoid UNIQUE constraint collisions during in-place changes
    cur.execute(
        "UPDATE activity SET activity_uid = 'TMP_' || id WHERE soa_id=?",
        (soa_id,),
    )
    cur.execute(
        "UPDATE activity SET activity_uid = 'Activity_' || order_index WHERE soa_id=?",
        (soa_id,),
    )
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "activity", old_order, order)
    # Activity-level audit entry capturing each id's order change list
    reorder_details = [
        {
            "id": aid,
            "before_order_index": before_rows.get(aid),
            "after_order_index": after_rows.get(aid),
        }
        for aid in order
    ]
    _record_activity_audit(
        soa_id,
        "reorder",
        activity_id=None,
        before={"old_order": old_order},
        after={"new_order": order, "details": reorder_details},
    )
    return JSONResponse({"ok": True, "old_order": old_order, "new_order": order})


@app.post("/soa/{soa_id}/epochs/reorder", response_class=JSONResponse)
def reorder_epochs_api(soa_id: int, order: List[int]):
    """JSON reorder endpoint for epochs. Records both global reorder audit and epoch_audit 'reorder' entry."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    if not order:
        raise HTTPException(400, "Order list required")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM epoch WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM epoch WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(order) - existing:
        conn.close()
        raise HTTPException(400, "Order contains invalid epoch id")
    for idx, eid in enumerate(order, start=1):
        cur.execute("UPDATE epoch SET order_index=? WHERE id=?", (idx, eid))
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "epoch", old_order, order)
    # Epoch-specific audit entry similar to element reorder
    _record_epoch_audit(
        soa_id,
        "reorder",
        epoch_id=None,
        before={"old_order": old_order},
        after={"new_order": order},
    )
    return JSONResponse({"ok": True, "old_order": old_order, "new_order": order})


class EpochCreate(BaseModel):
    name: str
    epoch_label: Optional[str] = None
    epoch_description: Optional[str] = None


class EpochUpdate(BaseModel):
    name: Optional[str] = None
    epoch_label: Optional[str] = None
    epoch_description: Optional[str] = None


class ConceptsUpdate(BaseModel):
    concept_codes: List[str]


class FreezeCreate(BaseModel):
    version_label: Optional[str] = None


def _list_freezes(soa_id: int):
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id, version_label, created_at FROM soa_freeze WHERE soa_id=? ORDER BY id DESC",
        (soa_id,),
    )
    rows = [dict(id=r[0], version_label=r[1], created_at=r[2]) for r in cur.fetchall()]
    conn.close()
    return rows


def _get_freeze(soa_id: int, freeze_id: int):
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id, version_label, created_at, snapshot_json FROM soa_freeze WHERE id=? AND soa_id=?",
        (freeze_id, soa_id),
    )
    row = cur.fetchone()
    conn.close()
    if not row:
        return None
    try:
        snap = json.loads(row[3])
    except Exception:
        snap = {"error": "Corrupt snapshot"}
    return {
        "id": row[0],
        "version_label": row[1],
        "created_at": row[2],
        "snapshot": snap,
    }


def _create_freeze(soa_id: int, version_label: Optional[str]):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    # Auto version label if not provided
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT version_label FROM soa_freeze WHERE soa_id=?", (soa_id,))
    existing_labels = {r[0] for r in cur.fetchall()}
    if not version_label or not version_label.strip():
        # Find next available vN
        n = 1
        while f"v{n}" in existing_labels:
            n += 1
        version_label = f"v{n}"
    else:
        version_label = version_label.strip()
    if version_label in existing_labels:
        raise HTTPException(400, "Version label already exists for this SOA")
    # Gather snapshot data
    cur.execute(
        "SELECT name, created_at, study_id, study_label, study_description FROM soa WHERE id=?",
        (soa_id,),
    )
    row = cur.fetchone()
    soa_name = row[0] if row else f"SOA {soa_id}"
    study_id_val = row[2] if row else None
    study_label_val = row[3] if row else None
    study_description_val = row[4] if row else None
    visits, activities, cells = _fetch_matrix(soa_id)
    # Epochs snapshot (ordered)
    conn2 = _connect()
    cur2 = conn2.cursor()
    cur2.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    epochs = [
        dict(
            id=r[0],
            name=r[1],
            order_index=r[2],
            epoch_seq=r[3],
            epoch_label=r[4],
            epoch_description=r[5],
        )
        for r in cur2.fetchall()
    ]
    conn2.close()
    # Elements snapshot (ordered)
    conn_el = _connect()
    cur_el = conn_el.cursor()
    cur_el.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index FROM element WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    elements = [
        dict(
            id=r[0],
            name=r[1],
            label=r[2],
            description=r[3],
            testrl=r[4],
            teenrl=r[5],
            order_index=r[6],
        )
        for r in cur_el.fetchall()
    ]
    conn_el.close()
    # Concept mapping
    activity_ids = [a["id"] for a in activities]
    concepts_map = {}
    if activity_ids:
        placeholders = ",".join("?" for _ in activity_ids)
        cur.execute(
            f"SELECT activity_id, concept_code, concept_title FROM activity_concept WHERE activity_id IN ({placeholders})",
            activity_ids,
        )
        for aid, code, title in cur.fetchall():
            concepts_map.setdefault(aid, []).append({"code": code, "title": title})
    snapshot = {
        "soa_id": soa_id,
        "soa_name": soa_name,
        "study_id": study_id_val,
        "study_label": study_label_val,
        "study_description": study_description_val,
        "version_label": version_label,
        "frozen_at": datetime.now(timezone.utc).isoformat(),
        "epochs": epochs,
        "elements": elements,
        "visits": visits,
        "activities": activities,
        "cells": cells,
        "activity_concepts": concepts_map,
    }
    snap_json = json.dumps(snapshot)
    cur.execute(
        "INSERT INTO soa_freeze (soa_id, version_label, created_at, snapshot_json) VALUES (?,?,?,?)",
        (soa_id, version_label, datetime.now(timezone.utc).isoformat(), snap_json),
    )
    fid = cur.lastrowid
    conn.commit()
    conn.close()
    return fid, version_label


def _diff_freezes(soa_id: int, left_id: int, right_id: int):
    return _diff_freezes_limited(soa_id, left_id, right_id, limit=None)


def _diff_freezes_limited(
    soa_id: int, left_id: int, right_id: int, limit: Optional[int]
):
    left = _get_freeze(soa_id, left_id)
    right = _get_freeze(soa_id, right_id)
    if not left or not right:
        raise HTTPException(404, "Freeze not found")
    l_snap = left["snapshot"]
    r_snap = right["snapshot"]
    # Visits
    l_vis = {
        str(v["id"]): v
        for v in l_snap.get("visits", [])
        if isinstance(v, dict) and "id" in v
    }
    r_vis = {
        str(v["id"]): v
        for v in r_snap.get("visits", [])
        if isinstance(v, dict) and "id" in v
    }
    visits_added_all = [r_vis[k] for k in r_vis.keys() - l_vis.keys()]
    visits_removed_all = [l_vis[k] for k in l_vis.keys() - r_vis.keys()]
    # Activities
    l_act = {
        str(a["id"]): a
        for a in l_snap.get("activities", [])
        if isinstance(a, dict) and "id" in a
    }
    r_act = {
        str(a["id"]): a
        for a in r_snap.get("activities", [])
        if isinstance(a, dict) and "id" in a
    }
    acts_added_all = [r_act[k] for k in r_act.keys() - l_act.keys()]
    acts_removed_all = [l_act[k] for k in l_act.keys() - r_act.keys()]
    # Cells (status changes)
    l_cells = {
        (c["visit_id"], c["activity_id"]): c
        for c in l_snap.get("cells", [])
        if isinstance(c, dict)
    }
    r_cells = {
        (c["visit_id"], c["activity_id"]): c
        for c in r_snap.get("cells", [])
        if isinstance(c, dict)
    }
    cells_added_all = [r_cells[k] for k in r_cells.keys() - l_cells.keys()]
    cells_removed_all = [l_cells[k] for k in l_cells.keys() - r_cells.keys()]
    cells_changed_all = []
    for k in r_cells.keys() & l_cells.keys():
        if r_cells[k].get("status") != l_cells[k].get("status"):
            cells_changed_all.append(
                {
                    "visit_id": k[0],
                    "activity_id": k[1],
                    "old_status": l_cells[k].get("status"),
                    "new_status": r_cells[k].get("status"),
                }
            )
    # Concepts per activity with title change detection
    l_concepts_map = l_snap.get("activity_concepts", {}) or {}
    r_concepts_map = r_snap.get("activity_concepts", {}) or {}
    concept_changes_all = []
    all_aids = set(map(str, l_concepts_map.keys())) | set(
        map(str, r_concepts_map.keys())
    )

    def _get_concept_list(m, key):
        # Support snapshots where JSON serialization converted int keys to strings
        if key in m:
            return m[key] or []
        if key.isdigit() and int(key) in m:
            return m[int(key)] or []
        return []

    for aid in all_aids:
        la = _get_concept_list(l_concepts_map, aid)
        ra = _get_concept_list(r_concepts_map, aid)
        l_set = {c["code"] for c in la if isinstance(c, dict)}
        r_set = {c["code"] for c in ra if isinstance(c, dict)}
        added = sorted(list(r_set - l_set))
        removed = sorted(list(l_set - r_set))
        title_changes = []
        for code in sorted(list(l_set & r_set)):
            l_title = next((c["title"] for c in la if c.get("code") == code), None)
            r_title = next((c["title"] for c in ra if c.get("code") == code), None)
            if l_title is not None and r_title is not None and l_title != r_title:
                title_changes.append(
                    {"code": code, "old_title": l_title, "new_title": r_title}
                )
        if added or removed or title_changes:
            concept_changes_all.append(
                {
                    "activity_id": aid,
                    "added": added,
                    "removed": removed,
                    "title_changes": title_changes,
                }
            )

    # Apply limit truncation if provided and >0
    def _truncate(lst):
        if limit and limit > 0 and len(lst) > limit:
            return lst[:limit], True
        return lst, False

    visits_added, visits_added_trunc = _truncate(visits_added_all)
    visits_removed, visits_removed_trunc = _truncate(visits_removed_all)
    acts_added, acts_added_trunc = _truncate(acts_added_all)
    acts_removed, acts_removed_trunc = _truncate(acts_removed_all)
    cells_added, cells_added_trunc = _truncate(cells_added_all)
    cells_removed, cells_removed_trunc = _truncate(cells_removed_all)
    cells_changed, cells_changed_trunc = _truncate(cells_changed_all)
    concept_changes, concept_changes_trunc = _truncate(concept_changes_all)
    meta = {
        "limit": limit,
        "visits": {
            "added_total": len(visits_added_all),
            "removed_total": len(visits_removed_all),
            "added_truncated": visits_added_trunc,
            "removed_truncated": visits_removed_trunc,
        },
        "activities": {
            "added_total": len(acts_added_all),
            "removed_total": len(acts_removed_all),
            "added_truncated": acts_added_trunc,
            "removed_truncated": acts_removed_trunc,
        },
        "cells": {
            "added_total": len(cells_added_all),
            "removed_total": len(cells_removed_all),
            "changed_total": len(cells_changed_all),
            "added_truncated": cells_added_trunc,
            "removed_truncated": cells_removed_trunc,
            "changed_truncated": cells_changed_trunc,
        },
        "concepts": {
            "changes_total": len(concept_changes_all),
            "changes_truncated": concept_changes_trunc,
        },
    }
    return {
        "left": {
            "id": left["id"],
            "label": left["version_label"],
            "created_at": left["created_at"],
        },
        "right": {
            "id": right["id"],
            "label": right["version_label"],
            "created_at": right["created_at"],
        },
        "visits": {"added": visits_added, "removed": visits_removed},
        "activities": {"added": acts_added, "removed": acts_removed},
        "cells": {
            "added": cells_added,
            "removed": cells_removed,
            "changed": cells_changed,
        },
        "concepts": concept_changes,
        "meta": meta,
    }


def _rollback_freeze(soa_id: int, freeze_id: int) -> dict:
    freeze = _get_freeze(soa_id, freeze_id)
    if not freeze:
        raise HTTPException(404, "Freeze not found")
    snap = freeze["snapshot"]
    if snap.get("soa_id") != soa_id:
        raise HTTPException(400, "Snapshot SoA mismatch")
    visits = snap.get("visits", [])
    activities = snap.get("activities", [])
    cells = snap.get("cells", [])
    elements = snap.get("elements", [])
    concepts_map = snap.get("activity_concepts", {}) or {}
    conn = _connect()
    cur = conn.cursor()
    # Clear existing
    # Order matters: delete cells, then concepts (while activity rows still exist), then activities, then visits.
    cur.execute("DELETE FROM cell WHERE soa_id=?", (soa_id,))
    cur.execute(
        "DELETE FROM activity_concept WHERE activity_id IN (SELECT id FROM activity WHERE soa_id=? )",
        (soa_id,),
    )
    cur.execute("DELETE FROM activity WHERE soa_id=?", (soa_id,))
    cur.execute("DELETE FROM visit WHERE soa_id=?", (soa_id,))
    cur.execute("DELETE FROM element WHERE soa_id=?", (soa_id,))
    # Reinsert visits mapping old id->new id
    visit_id_map = {}
    for v in sorted(visits, key=lambda x: x.get("order_index", 0)):
        cur.execute(
            "INSERT INTO visit (soa_id,name,raw_header,order_index) VALUES (?,?,?,?)",
            (
                soa_id,
                v.get("name"),
                v.get("raw_header") or v.get("name"),
                v.get("order_index"),
            ),
        )
        new_id = cur.lastrowid
        visit_id_map[v.get("id")] = new_id
    # Reinsert activities mapping old id->new id
    activity_id_map = {}
    for a in sorted(activities, key=lambda x: x.get("order_index", 0)):
        cur.execute(
            "INSERT INTO activity (soa_id,name,order_index) VALUES (?,?,?)",
            (soa_id, a.get("name"), a.get("order_index")),
        )
        new_id = cur.lastrowid
        activity_id_map[a.get("id")] = new_id
    # Reinsert cells
    inserted_cells = 0
    for c in cells:
        old_vid = c.get("visit_id")
        old_aid = c.get("activity_id")
        status = c.get("status", "").strip()
        if status == "":
            continue
        vid = visit_id_map.get(old_vid)
        aid = activity_id_map.get(old_aid)
        if vid and aid:
            cur.execute(
                "INSERT INTO cell (soa_id, visit_id, activity_id, status) VALUES (?,?,?,?)",
                (soa_id, vid, aid, status),
            )
            inserted_cells += 1
    # Reinsert concepts
    # Reinsert elements
    elements_restored = 0
    for el in sorted(elements, key=lambda x: x.get("order_index", 0)):
        cur.execute(
            "INSERT INTO element (soa_id,name,label,description,testrl,teenrl,order_index,created_at) VALUES (?,?,?,?,?,?,?,?)",
            (
                soa_id,
                el.get("name"),
                el.get("label"),
                el.get("description"),
                el.get("testrl"),
                el.get("teenrl"),
                el.get("order_index"),
                datetime.utcnow().isoformat(),
            ),
        )
        elements_restored += 1
    inserted_concepts = 0
    for old_aid, concept_list in concepts_map.items():
        new_aid = activity_id_map.get(int(old_aid))
        if not new_aid:
            continue
        for c in concept_list:
            code = c.get("code")
            title = c.get("title") or code
            if not code:
                continue
            cur.execute(
                "INSERT INTO activity_concept (activity_id, concept_code, concept_title) VALUES (?,?,?)",
                (new_aid, code, title),
            )
            inserted_concepts += 1
    conn.commit()
    conn.close()
    return {
        "rollback_freeze_id": freeze_id,
        "visits_restored": len(visits),
        "activities_restored": len(activities),
        "cells_restored": inserted_cells,
        "concept_mappings_restored": inserted_concepts,
        "elements_restored": elements_restored,
    }


def _record_rollback_audit(soa_id: int, freeze_id: int, stats: dict):
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "INSERT INTO rollback_audit (soa_id, freeze_id, performed_at, visits_restored, activities_restored, cells_restored, concepts_restored, elements_restored) VALUES (?,?,?,?,?,?,?,?)",
        (
            soa_id,
            freeze_id,
            datetime.now(timezone.utc).isoformat(),
            stats.get("visits_restored"),
            stats.get("activities_restored"),
            stats.get("cells_restored"),
            stats.get("concept_mappings_restored"),
            stats.get("elements_restored"),
        ),
    )
    conn.commit()
    conn.close()


def _record_reorder_audit(
    soa_id: int, entity_type: str, old_order: list[int], new_order: list[int]
):
    """Persist a reorder audit record if ordering truly changed.

    Parameters:
      soa_id: owning SoA id
            entity_type: 'visit' | 'activity' | 'epoch'
      old_order: list of IDs before reorder (ascending order_index)
      new_order: list of IDs after reorder (ascending order_index)
    """
    try:
        if old_order == new_order:
            return  # no change
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "INSERT INTO reorder_audit (soa_id, entity_type, old_order_json, new_order_json, performed_at) VALUES (?,?,?,?,?)",
            (
                soa_id,
                entity_type,
                json.dumps(old_order),
                json.dumps(new_order),
                datetime.now(timezone.utc).isoformat(),
            ),
        )
        conn.commit()
        conn.close()
    except (
        Exception
    ) as e:  # pragma: no cover - audit failure should not break core flow
        logger.warning("Failed to record reorder audit: %s", e)


def _list_reorder_audit(soa_id: int) -> list[dict]:
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id, entity_type, old_order_json, new_order_json, performed_at FROM reorder_audit WHERE soa_id=? ORDER BY id DESC",
        (soa_id,),
    )
    rows = [
        {
            "id": r[0],
            "entity_type": r[1],
            "old_order": json.loads(r[2]) if r[2] else [],
            "new_order": json.loads(r[3]) if r[3] else [],
            "performed_at": r[4],
        }
        for r in cur.fetchall()
    ]
    conn.close()
    return rows


def _fetch_arms_for_edit(soa_id: int) -> list[dict]:
    """Return ordered arms for edit template."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "SELECT id,name,label,description,order_index FROM arm WHERE soa_id=? ORDER BY order_index",
            (soa_id,),
        )
        rows = [
            {
                "id": r[0],
                "name": r[1],
                "label": r[2],
                "description": r[3],
                "order_index": r[4],
            }
            for r in cur.fetchall()
        ]
        conn.close()
        return rows
    except Exception:
        return []


def _list_rollback_audit(soa_id: int) -> list[dict]:
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id, freeze_id, performed_at, visits_restored, activities_restored, cells_restored, concepts_restored FROM rollback_audit WHERE soa_id=? ORDER BY id DESC",
        (soa_id,),
    )
    rows = [
        {
            "id": r[0],
            "freeze_id": r[1],
            "performed_at": r[2],
            "visits_restored": r[3],
            "activities_restored": r[4],
            "cells_restored": r[5],
            "concepts_restored": r[6],
        }
        for r in cur.fetchall()
    ]
    conn.close()
    return rows


def _rollback_preview(soa_id: int, freeze_id: int) -> dict:
    freeze = _get_freeze(soa_id, freeze_id)
    if not freeze:
        raise HTTPException(404, "Freeze not found")
    snap = freeze["snapshot"]
    visits = snap.get("visits", [])
    activities = snap.get("activities", [])
    cells = [c for c in snap.get("cells", []) if c.get("status", "").strip() != ""]
    concepts_map = snap.get("activity_concepts", {}) or {}
    return {
        "freeze_id": freeze_id,
        "version_label": freeze.get("version_label"),
        "visits_to_restore": len(visits),
        "activities_to_restore": len(activities),
        "cells_to_restore": len(cells),
        "concept_mappings_to_restore": sum(len(v) for v in concepts_map.values()),
    }


class CellCreate(BaseModel):
    visit_id: int
    activity_id: int
    status: str


class BulkActivities(BaseModel):
    names: List[str]


class MatrixVisit(BaseModel):
    name: str
    raw_header: Optional[str] = None


class MatrixActivity(BaseModel):
    name: str
    statuses: List[str]


class MatrixImport(BaseModel):
    visits: List[MatrixVisit]
    activities: List[MatrixActivity]
    reset: bool = True


# --------------------- Helpers ---------------------


def _soa_exists(soa_id: int) -> bool:
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM soa WHERE id=?", (soa_id,))
    row = cur.fetchone()
    conn.close()
    return row is not None


def _fetch_matrix(soa_id: int):
    conn = _connect()
    cur = conn.cursor()
    # Epochs not part of matrix axes currently; retrieved separately where needed.
    cur.execute(
        "SELECT id,name,raw_header,order_index,epoch_id FROM visit WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    visits = [
        dict(id=r[0], name=r[1], raw_header=r[2], order_index=r[3], epoch_id=r[4])
        for r in cur.fetchall()
    ]
    cur.execute(
        "SELECT id,name,order_index FROM activity WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    activities = [dict(id=r[0], name=r[1], order_index=r[2]) for r in cur.fetchall()]
    cur.execute(
        "SELECT visit_id, activity_id, status FROM cell WHERE soa_id=?", (soa_id,)
    )
    cells = [dict(visit_id=r[0], activity_id=r[1], status=r[2]) for r in cur.fetchall()]
    conn.close()
    return visits, activities, cells


def fetch_biomedical_concepts(force: bool = False):
    """Return list of biomedical concepts as [{'code':..., 'title':...}].
    Precedence: CDISC_CONCEPTS_JSON env override (for tests/offline) > cached remote fetch > empty list.
    Remote fetch uses CDISC_API_KEY header if present. Caches for TTL duration.
    """
    now = time.time()
    if (
        not force
        and _concept_cache["data"]
        and now - _concept_cache["fetched_at"] < _CONCEPT_CACHE_TTL
    ):
        return _concept_cache["data"]
    # Environment override
    override_json = _get_concepts_override()
    if override_json:
        try:
            raw = json.loads(override_json)
            items = (
                raw.get("items") if isinstance(raw, dict) and "items" in raw else raw
            )
            concepts = []
            for it in items:
                code = (
                    it.get("concept_code")
                    or it.get("code")
                    or it.get("conceptId")
                    or it.get("id")
                    or it.get("identifier")
                )
                title = it.get("title") or it.get("name") or it.get("label") or code
                if code:
                    concepts.append({"code": str(code), "title": str(title)})
            concepts.sort(key=lambda c: c["title"].lower())
            _concept_cache.update(data=concepts, fetched_at=now)
            logger.info("Loaded %d concepts from env override", len(concepts))
            return concepts
        except Exception:
            pass
    # Remote
    if os.environ.get("CDISC_SKIP_REMOTE") == "1":
        _concept_cache.update(data=[], fetched_at=now)
        logger.warning("CDISC_SKIP_REMOTE=1; concept list empty")
        return []
    url = "https://api.library.cdisc.org/api/cosmos/v2/mdr/bc/biomedicalconcepts"
    headers = {"Accept": "application/json"}
    api_key = _get_cdisc_api_key()
    subscription_key = os.environ.get("CDISC_SUBSCRIPTION_KEY") or api_key
    # Some CDISC gateways require subscription key header, others accept bearer/api-key; send all when available.
    if subscription_key:
        headers["Ocp-Apim-Subscription-Key"] = subscription_key
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"  # bearer token style
        headers["api-key"] = api_key  # fallback header name
    try:
        resp = requests.get(url, headers=headers, timeout=15)
        _concept_cache["last_status"] = resp.status_code
        _concept_cache["last_url"] = url
        _concept_cache["last_error"] = None
        _concept_cache["raw_snippet"] = resp.text[:400]
        if resp.status_code == 200:
            try:
                data = resp.json()
            except ValueError:
                # Not JSON, likely HTML error despite 200
                _concept_cache["last_error"] = "200 but non-JSON response"
                logger.error(
                    "Concept fetch 200 but non-JSON body (snippet: %s)", resp.text[:200]
                )
                return []

            # If JSON is a string, attempt second decode
            if isinstance(data, str):
                try:
                    data = json.loads(data)
                except Exception:
                    _concept_cache["last_error"] = (
                        "JSON value was a raw string; secondary parse failed"
                    )
                    logger.error(
                        "Concept fetch raw string JSON secondary parse failed (snippet: %s)",
                        str(data)[:200],
                    )
                    return []

            # Normalize possible shapes
            # Primary shapes: list of concept objects, dict with 'items', or HAL-style _links
            if (
                isinstance(data, dict)
                and "items" in data
                and isinstance(data["items"], list)
            ):
                items = data["items"]
            elif (
                isinstance(data, dict)
                and "_links" in data
                and isinstance(data["_links"], dict)
            ):
                # Extract from biomedicalConcepts links list
                links_list = data["_links"].get("biomedicalConcepts") or []
                items = []
                for link in links_list:
                    if not isinstance(link, dict):
                        continue
                    href = link.get("href")
                    title = link.get("title") or href
                    # Concept code may be last path segment
                    code = None
                    if href:
                        code = href.strip("/").split("/")[-1]
                    if code:
                        items.append({"concept_code": code, "title": title})
            elif isinstance(data, list):
                items = data
            elif isinstance(data, dict):
                # single concept object
                items = [data]
            else:
                _concept_cache["last_error"] = (
                    f"Unexpected JSON root type: {type(data).__name__}"
                )
                logger.error("Concept fetch unexpected JSON root type: %s", type(data))
                return []

            concepts = []
            for it in items:
                if not isinstance(it, dict):
                    continue  # skip non-dict entries
                code = (
                    it.get("concept_code")
                    or it.get("code")
                    or it.get("conceptId")
                    or it.get("id")
                    or it.get("identifier")
                )
                title = it.get("title") or it.get("name") or it.get("label") or code
                if code:
                    concepts.append({"code": str(code), "title": str(title)})
            concepts.sort(key=lambda c: c["title"].lower())
            _concept_cache.update(data=concepts, fetched_at=now)
            logger.info("Fetched %d concepts from remote API", len(concepts))
            return concepts
        else:
            _concept_cache["last_error"] = f"HTTP {resp.status_code}: {resp.text[:200]}"
    except Exception as e:
        logger.error("Concept fetch error: %s", e)
        _concept_cache["last_error"] = str(e)
    _concept_cache.update(data=[], fetched_at=now)
    logger.warning("Concept list empty after fetch attempts")
    return []


def fetch_sdtm_specializations(force: bool = False):
    """Return list of SDTM dataset specializations as [{'title':..., 'href':...}].
    Remote precedence similar to concepts. Supports optional env override CDISC_SDTM_SPECIALIZATIONS_JSON for tests/offline.
    Dates removed for simpler UI; results sorted alphabetically by title.
    """
    now = time.time()
    if (
        not force
        and _sdtm_specializations_cache["data"]
        and now - _sdtm_specializations_cache["fetched_at"]
        < _SDTM_SPECIALIZATIONS_CACHE_TTL
    ):
        return _sdtm_specializations_cache["data"]

    override_json = os.environ.get("CDISC_SDTM_SPECIALIZATIONS_JSON")
    base_prefix = "https://api.library.cdisc.org/api/cosmos/v2"
    if override_json:
        try:
            raw = json.loads(override_json)
            if isinstance(raw, dict):
                if "items" in raw and isinstance(raw["items"], list):
                    items = raw["items"]
                elif "datasetSpecializations" in raw and isinstance(
                    raw["datasetSpecializations"], dict
                ):
                    items = list(raw["datasetSpecializations"].values())
                else:
                    items = [raw]
            elif isinstance(raw, list):
                items = raw
            else:
                items = []
            packages: list[dict] = []
            for it in items:
                if not isinstance(it, dict):
                    continue
                title_keys = [
                    "title",
                    "name",
                    "label",
                    "datasetLabel",
                    "datasetName",
                    "datasetSpecializationLabel",
                    "datasetSpecializationName",
                ]
                title = next((it.get(k) for k in title_keys if it.get(k)), "(untitled)")
                href = it.get("href") or it.get("link")
                if not href:
                    id_val = (
                        it.get("id")
                        or it.get("datasetSpecializationId")
                        or it.get("code")
                    )
                    if id_val:
                        href = (
                            "https://api.library.cdisc.org/api/cosmos/v2/mdr/specializations/sdtm/datasetspecializations/"
                            + str(id_val)
                        )
                # Normalize relative/partial href to absolute
                if (
                    href
                    and not href.startswith("http://")
                    and not href.startswith("https://")
                ):
                    if href.startswith("/"):
                        href = base_prefix + href
                    else:
                        href = base_prefix + "/" + href
                packages.append({"title": title, "href": href})
            packages.sort(key=lambda p: p.get("title", "").lower())
            _sdtm_specializations_cache.update(data=packages, fetched_at=now)
            logger.info(
                "Loaded %d SDTM dataset specializations from override", len(packages)
            )
            return packages
        except Exception:
            pass

    if os.environ.get("CDISC_SKIP_REMOTE") == "1":
        _sdtm_specializations_cache.update(data=[], fetched_at=now)
        logger.warning("CDISC_SKIP_REMOTE=1; SDTM dataset specializations list empty")
        return []

    url = "https://api.library.cdisc.org/api/cosmos/v2/mdr/specializations/sdtm/datasetspecializations"
    headers = {"Accept": "application/json"}
    api_key = _get_cdisc_api_key()
    subscription_key = os.environ.get("CDISC_SUBSCRIPTION_KEY") or api_key
    if subscription_key:
        headers["Ocp-Apim-Subscription-Key"] = subscription_key
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
        headers["api-key"] = api_key

    packages: list[dict] = []
    try:
        resp = requests.get(url, headers=headers, timeout=20)
        _sdtm_specializations_cache["last_status"] = resp.status_code
        _sdtm_specializations_cache["last_url"] = url
        _sdtm_specializations_cache["last_error"] = None
        _sdtm_specializations_cache["raw_snippet"] = resp.text[:400]
        if resp.status_code == 200:
            try:
                data = resp.json()
            except ValueError:
                _sdtm_specializations_cache["last_error"] = "200 but non-JSON response"
                data = None
            if isinstance(data, str):
                try:
                    data = json.loads(data)
                except Exception:
                    _sdtm_specializations_cache["last_error"] = (
                        "Raw string JSON secondary parse failed"
                    )
                    data = None
            items = []
            if isinstance(data, dict):
                if "items" in data and isinstance(data["items"], list):
                    items = data["items"]
                elif "_links" in data and isinstance(data["_links"], dict):
                    link_list = []
                    for key in (
                        "datasetSpecializations",
                        "datasetspecializations",
                        "packages",
                    ):
                        val = data["_links"].get(key)
                        if isinstance(val, list):
                            link_list = val
                            break
                    for link in link_list:
                        if not isinstance(link, dict):
                            continue
                        href = link.get("href")
                        title = link.get("title") or href
                        if (
                            href
                            and not href.startswith("http://")
                            and not href.startswith("https://")
                        ):
                            if href.startswith("/"):
                                href = base_prefix + href
                            else:
                                href = base_prefix + "/" + href
                        packages.append({"title": title, "href": href})
                elif "datasetSpecializations" in data and isinstance(
                    data["datasetSpecializations"], dict
                ):
                    items = list(data["datasetSpecializations"].values())
                else:
                    items = [data]
            elif isinstance(data, list):
                items = data
            if items:
                for it in items:
                    if not isinstance(it, dict):
                        continue
                    title_keys = [
                        "title",
                        "name",
                        "label",
                        "datasetLabel",
                        "datasetName",
                        "datasetSpecializationLabel",
                        "datasetSpecializationName",
                    ]
                    title = next(
                        (it.get(k) for k in title_keys if it.get(k)), "(untitled)"
                    )
                    href = it.get("href") or it.get("link")
                    if not href:
                        id_val = (
                            it.get("id")
                            or it.get("datasetSpecializationId")
                            or it.get("code")
                        )
                        if id_val:
                            href = f"{url}/{id_val}"
                    if (
                        href
                        and not href.startswith("http://")
                        and not href.startswith("https://")
                    ):
                        if href.startswith("/"):
                            href = base_prefix + href
                        else:
                            href = base_prefix + "/" + href
                    packages.append({"title": title, "href": href})
        else:
            _sdtm_specializations_cache["last_error"] = (
                f"HTTP {resp.status_code}: {resp.text[:180]}"
            )
    except Exception as e:
        logger.error("SDTM dataset specializations fetch error: %s", e)
        _sdtm_specializations_cache["last_error"] = str(e)

    packages.sort(key=lambda p: p.get("title", "").lower())
    _sdtm_specializations_cache.update(data=packages, fetched_at=now)
    logger.info(
        "Fetched %d SDTM dataset specializations from remote API", len(packages)
    )
    return packages


@app.on_event("startup")
def preload_concepts():  # pragma: no cover (covered indirectly via tests reload)
    """Preload cached terminology datasets on service startup.

    Fetches biomedical concepts and SDTM dataset specializations so first request
    hits warm caches. Errors are logged but not raised (startup should proceed).
    """
    try:
        concepts = fetch_biomedical_concepts(force=True)
        logger.info("Startup preload concepts count=%d", len(concepts))
    except Exception as e:
        logger.error("Startup concept preload failed: %s", e)
    try:
        sdtm_specs = fetch_sdtm_specializations(force=True)
        logger.info("Startup preload SDTM specializations count=%d", len(sdtm_specs))
    except Exception as e:
        logger.error("Startup SDTM specializations preload failed: %s", e)


@app.post("/ui/soa/{soa_id}/concepts_refresh")
def ui_refresh_concepts(request: Request, soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    fetch_biomedical_concepts(force=True)
    # If HTMX request, use HX-Redirect header for clean redirect without injecting script
    if request.headers.get("HX-Request") == "true":
        return HTMLResponse("", headers={"HX-Redirect": f"/ui/soa/{soa_id}/edit"})
    # Fallback: plain form POST non-htmx redirect via script
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/freeze", response_class=HTMLResponse)
def ui_freeze_soa(request: Request, soa_id: int, version_label: str = Form("")):
    try:
        _fid, _vlabel = _create_freeze(soa_id, version_label or None)
    except HTTPException as he:
        # Return inline error block for HTMX; simple alert fallback for non-HTMX
        if request.headers.get("HX-Request") == "true":
            return HTMLResponse(
                f"<div class='error' style='color:#c62828;font-size:0.7em;'>Error: {he.detail}</div>"
            )
        return HTMLResponse(
            f"<script>alert('Error: {he.detail}');window.location='/ui/soa/{soa_id}/edit';</script>"
        )
    if request.headers.get("HX-Request") == "true":
        return HTMLResponse("", headers={"HX-Redirect": f"/ui/soa/{soa_id}/edit"})
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.get("/soa/{soa_id}/freeze/{freeze_id}")
def get_freeze(soa_id: int, freeze_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT snapshot_json FROM soa_freeze WHERE id=? AND soa_id=?",
        (freeze_id, soa_id),
    )
    row = cur.fetchone()
    conn.close()
    if not row:
        raise HTTPException(404, "Freeze not found")
    try:
        data = json.loads(row[0])
    except Exception:
        data = {"error": "Corrupt snapshot"}
    return JSONResponse(data)


@app.get("/ui/soa/{soa_id}/freeze/{freeze_id}/view", response_class=HTMLResponse)
def ui_freeze_view(request: Request, soa_id: int, freeze_id: int):
    freeze = _get_freeze(soa_id, freeze_id)
    if not freeze:
        raise HTTPException(404, "Freeze not found")
    return templates.TemplateResponse(
        "freeze_modal.html",
        {"request": request, "mode": "view", "freeze": freeze, "soa_id": soa_id},
    )


@app.get("/ui/soa/{soa_id}/freeze/diff", response_class=HTMLResponse)
def ui_freeze_diff(request: Request, soa_id: int, left: int, right: int, full: int = 0):
    limit = None if full == 1 else 50
    diff = _diff_freezes_limited(soa_id, left, right, limit=limit)
    return templates.TemplateResponse(
        "freeze_modal.html",
        {"request": request, "mode": "diff", "diff": diff, "soa_id": soa_id},
    )


@app.post("/ui/soa/{soa_id}/freeze/{freeze_id}/rollback", response_class=HTMLResponse)
def ui_freeze_rollback(request: Request, soa_id: int, freeze_id: int):
    result = _rollback_freeze(soa_id, freeze_id)
    _record_rollback_audit(
        soa_id,
        freeze_id,
        {
            "visits_restored": result["visits_restored"],
            "activities_restored": result["activities_restored"],
            "cells_restored": result["cells_restored"],
            "concept_mappings_restored": result["concept_mappings_restored"],
        },
    )
    # HTMX redirect back to edit with status message injected if desired later
    if request.headers.get("HX-Request") == "true":
        return HTMLResponse("", headers={"HX-Redirect": f"/ui/soa/{soa_id}/edit"})
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.get(
    "/ui/soa/{soa_id}/freeze/{freeze_id}/rollback_preview", response_class=HTMLResponse
)
def ui_freeze_rollback_preview(request: Request, soa_id: int, freeze_id: int):
    preview = _rollback_preview(soa_id, freeze_id)
    freeze = _get_freeze(soa_id, freeze_id)
    return templates.TemplateResponse(
        "freeze_modal.html",
        {
            "request": request,
            "mode": "rollback_preview",
            "preview": preview,
            "freeze": freeze,
            "soa_id": soa_id,
        },
    )


@app.get("/soa/{soa_id}/freeze/diff.json")
def get_freeze_diff_json(soa_id: int, left: int, right: int, full: int = 0):
    limit = None if full == 1 else 1000  # large default for JSON
    diff = _diff_freezes_limited(soa_id, left, right, limit=limit)
    return JSONResponse(diff)


@app.get("/soa/{soa_id}/rollback_audit")
def get_rollback_audit_json(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    return {"audit": _list_rollback_audit(soa_id)}


@app.get("/soa/{soa_id}/reorder_audit")
def get_reorder_audit_json(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    return {"audit": _list_reorder_audit(soa_id)}


@app.get("/ui/soa/{soa_id}/rollback_audit", response_class=HTMLResponse)
def ui_rollback_audit(request: Request, soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    return templates.TemplateResponse(
        "rollback_audit_modal.html",
        {"request": request, "soa_id": soa_id, "audit": _list_rollback_audit(soa_id)},
    )


@app.get("/ui/soa/{soa_id}/reorder_audit", response_class=HTMLResponse)
def ui_reorder_audit(request: Request, soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    return templates.TemplateResponse(
        "reorder_audit_modal.html",
        {"request": request, "soa_id": soa_id, "audit": _list_reorder_audit(soa_id)},
    )


@app.get("/soa/{soa_id}/rollback_audit/export/xlsx")
def export_rollback_audit_xlsx(soa_id: int):
    """Export rollback audit history for the SoA to an Excel workbook."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    rows = _list_rollback_audit(soa_id)
    # Prepare DataFrame
    df = pd.DataFrame(rows)
    if df.empty:
        # Create empty frame with columns for consistency
        df = pd.DataFrame(
            columns=[
                "id",
                "freeze_id",
                "performed_at",
                "visits_restored",
                "activities_restored",
                "cells_restored",
                "concepts_restored",
            ]
        )
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="RollbackAudit")
    bio.seek(0)
    filename = f"soa_{soa_id}_rollback_audit.xlsx"
    return StreamingResponse(
        bio,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


@app.get("/soa/{soa_id}/reorder_audit/export/xlsx")
def export_reorder_audit_xlsx(soa_id: int):
    """Export reorder audit history (visit/activity reorders) to Excel."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    rows = _list_reorder_audit(soa_id)
    # Flatten old/new order arrays to strings for readability
    flat_rows = []
    for r in rows:
        moves = []
        old_pos = {vid: idx + 1 for idx, vid in enumerate(r.get("old_order", []))}
        new_order = r.get("new_order", [])
        for idx, vid in enumerate(new_order, start=1):
            op = old_pos.get(vid)
            if op and op != idx:
                moves.append(f"{vid}:{op}->{idx}")
        flat_rows.append(
            {
                "id": r.get("id"),
                "entity_type": r.get("entity_type"),
                "performed_at": r.get("performed_at"),
                "old_order": ",".join(map(str, r.get("old_order", []))),
                "new_order": ",".join(map(str, new_order)),
                "moves": "; ".join(moves) if moves else "",
            }
        )
    df = pd.DataFrame(flat_rows)
    if df.empty:
        df = pd.DataFrame(
            columns=[
                "id",
                "entity_type",
                "performed_at",
                "old_order",
                "new_order",
                "moves",
            ]
        )
    bio = io.BytesIO()
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        df.to_excel(writer, index=False, sheet_name="ReorderAudit")
    bio.seek(0)
    filename = f"soa_{soa_id}_reorder_audit.xlsx"
    return StreamingResponse(
        bio,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


@app.get("/soa/{soa_id}/reorder_audit/export/csv")
def export_reorder_audit_csv(soa_id: int):
    """Export reorder audit history to CSV."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    rows = _list_reorder_audit(soa_id)
    # Prepare CSV lines
    header = ["id", "entity_type", "performed_at", "old_order", "new_order", "moves"]
    output = io.StringIO()
    writer = csv.writer(output)
    writer.writerow(header)
    for r in rows:
        old_order = r.get("old_order", [])
        new_order = r.get("new_order", [])
        moves = []
        old_pos = {vid: idx + 1 for idx, vid in enumerate(old_order)}
        for idx, vid in enumerate(new_order, start=1):
            op = old_pos.get(vid)
            if op and op != idx:
                moves.append(f"{vid}:{op}->{idx}")
        writer.writerow(
            [
                r.get("id"),
                r.get("entity_type"),
                r.get("performed_at"),
                ",".join(map(str, old_order)),
                ",".join(map(str, new_order)),
                "; ".join(moves) if moves else "",
            ]
        )
    output.seek(0)
    filename = f"soa_{soa_id}_reorder_audit.csv"
    return StreamingResponse(
        io.BytesIO(output.getvalue().encode("utf-8")),
        media_type="text/csv",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


@app.get("/concepts/status")
def concepts_status():
    return {
        "count": len(_concept_cache.get("data") or []),
        "fetched_at": _concept_cache.get("fetched_at"),
        "cache_age_sec": (
            (time.time() - _concept_cache.get("fetched_at", 0))
            if _concept_cache.get("fetched_at")
            else None
        ),
        "last_status": _concept_cache.get("last_status"),
        "last_error": _concept_cache.get("last_error"),
        "raw_snippet": _concept_cache.get("raw_snippet"),
        "api_key_present": bool(_get_cdisc_api_key()),
        "override_present": bool(_get_concepts_override()),
        "skip_remote": os.environ.get("CDISC_SKIP_REMOTE") == "1",
    }


@app.get("/sdtm/specializations/status")
def sdtm_specializations_status():
    """Return diagnostics for SDTM dataset specializations fetch/cache."""
    data = _sdtm_specializations_cache.get("data") or []
    fetched_at = _sdtm_specializations_cache.get("fetched_at")
    age = (time.time() - fetched_at) if fetched_at else None
    sample = data[:3]
    return {
        "count": len(data),
        "fetched_at": fetched_at,
        "cache_age_sec": age,
        "last_status": _sdtm_specializations_cache.get("last_status"),
        "last_error": _sdtm_specializations_cache.get("last_error"),
        "last_url": _sdtm_specializations_cache.get("last_url"),
        "raw_snippet": _sdtm_specializations_cache.get("raw_snippet"),
        "api_key_present": bool(_get_cdisc_api_key()),
        "skip_remote": os.environ.get("CDISC_SKIP_REMOTE") == "1",
        "override_present": bool(os.environ.get("CDISC_SDTM_SPECIALIZATIONS_JSON")),
        "sample": sample,
    }


@app.post("/ui/sdtm/specializations/refresh", response_class=HTMLResponse)
def ui_sdtm_specializations_refresh(request: Request):
    """Force refresh of SDTM specializations cache and redirect back to list."""
    fetch_sdtm_specializations(force=True)
    # HX redirect support
    if request.headers.get("HX-Request") == "true":
        return HTMLResponse("", headers={"HX-Redirect": "/ui/sdtm/specializations"})
    return HTMLResponse("<script>window.location='/ui/sdtm/specializations';</script>")


def _wide_csv_path(soa_id: int) -> str:
    return os.path.join(tempfile.gettempdir(), f"soa_{soa_id}_wide.csv")


def _generate_wide_csv(soa_id: int) -> str:
    visits, activities, cells = _fetch_matrix(soa_id)
    if not visits or not activities:
        raise ValueError(
            "Cannot generate CSV: need at least one visit and one activity"
        )
    # Build matrix with first column Activity, subsequent visit headers using raw_header or name
    visit_headers = [v["raw_header"] or v["name"] for v in visits]
    matrix = []
    for a in activities:
        row = [a["name"]]
        for v in visits:
            match = next(
                (
                    c["status"]
                    for c in cells
                    if c["visit_id"] == v["id"] and c["activity_id"] == a["id"]
                ),
                "",
            )
            row.append(match)
        matrix.append(row)
    path = _wide_csv_path(soa_id)
    with open(path, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Activity"] + visit_headers)
        writer.writerows(matrix)
    return path


def _matrix_arrays(soa_id: int):
    """Return visit headers list and rows (activity name + statuses)."""
    visits, activities, cells = _fetch_matrix(soa_id)
    visit_headers = [v["raw_header"] or v["name"] for v in visits]
    cell_lookup = {(c["visit_id"], c["activity_id"]): c["status"] for c in cells}
    rows = []
    for a in activities:
        row = [a["name"]]
        for v in visits:
            row.append(cell_lookup.get((v["id"], a["id"]), ""))
        rows.append(row)
    return visit_headers, rows


# --------------------- API Endpoints ---------------------


@app.post("/soa")
def create_soa(payload: SOACreate):
    conn = _connect()
    cur = conn.cursor()
    # Enforce unique study_id if provided
    if payload.study_id and payload.study_id.strip():
        cur.execute("SELECT 1 FROM soa WHERE study_id=?", (payload.study_id.strip(),))
        if cur.fetchone():
            conn.close()
            raise HTTPException(400, "study_id already exists")
    cur.execute(
        "INSERT INTO soa (name, created_at, study_id, study_label, study_description) VALUES (?,?,?,?,?)",
        (
            payload.name,
            datetime.now(timezone.utc).isoformat(),
            (payload.study_id or "").strip() or None,
            (payload.study_label or "").strip() or None,
            (payload.study_description or "").strip() or None,
        ),
    )
    soa_id = cur.lastrowid
    conn.commit()
    conn.close()
    return {
        "id": soa_id,
        "name": payload.name,
        "study_id": payload.study_id,
        "study_label": payload.study_label,
        "study_description": payload.study_description,
    }


@app.get("/soa/{soa_id}")
def get_soa(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    visits, activities, cells = _fetch_matrix(soa_id)
    # Fetch epochs
    conn_ep = _connect()
    cur_ep = conn_ep.cursor()
    cur_ep.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    epochs = [
        dict(
            id=r[0],
            name=r[1],
            order_index=r[2],
            epoch_seq=r[3],
            epoch_label=r[4],
            epoch_description=r[5],
        )
        for r in cur_ep.fetchall()
    ]
    conn_ep.close()
    # Also include study metadata if present
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT study_id, study_label, study_description FROM soa WHERE id=?", (soa_id,)
    )
    meta_row = cur.fetchone()
    conn.close()
    study_meta = (
        {
            "study_id": meta_row[0],
            "study_label": meta_row[1],
            "study_description": meta_row[2],
        }
        if meta_row
        else {}
    )
    return {
        "id": soa_id,
        **study_meta,
        "epochs": epochs,
        "visits": visits,
        "activities": activities,
        "cells": cells,
    }


@app.post("/soa/{soa_id}/metadata")
def update_soa_metadata(soa_id: int, payload: SOAMetadataUpdate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    # Fetch current study_id to enforce non-blank persistence
    cur.execute("SELECT study_id FROM soa WHERE id=?", (soa_id,))
    row = cur.fetchone()
    current_study_id = row[0] if row else None
    proposed = (payload.study_id or "").strip()
    if proposed == "" and current_study_id:
        # Ignore clearing attempt  keep existing value
        new_study_id = current_study_id
    else:
        new_study_id = proposed or None
    if new_study_id:
        cur.execute(
            "SELECT id FROM soa WHERE study_id=? AND id<>?", (new_study_id, soa_id)
        )
        if cur.fetchone():
            conn.close()
            raise HTTPException(400, "study_id already exists")
    # If there was no previous study_id and none provided now, reject
    if not current_study_id and not new_study_id:
        conn.close()
        raise HTTPException(400, "study_id is required and cannot be blank")
    cur.execute(
        "UPDATE soa SET study_id=?, study_label=?, study_description=? WHERE id=?",
        (
            new_study_id,
            (payload.study_label or "").strip() or None,
            (payload.study_description or "").strip() or None,
            soa_id,
        ),
    )
    conn.commit()
    conn.close()
    return {"id": soa_id, "updated": True}


@app.post("/soa/{soa_id}/visits")
def add_visit(soa_id: int, payload: VisitCreate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM visit WHERE soa_id=?", (soa_id,))
    order_index = cur.fetchone()[0] + 1
    if payload.epoch_id is not None:
        cur.execute(
            "SELECT 1 FROM epoch WHERE id=? AND soa_id=?", (payload.epoch_id, soa_id)
        )
        if not cur.fetchone():
            conn.close()
            raise HTTPException(400, "Invalid epoch_id for this SOA")
    cur.execute(
        "INSERT INTO visit (soa_id,name,raw_header,order_index,epoch_id) VALUES (?,?,?,?,?)",
        (
            soa_id,
            payload.name,
            payload.raw_header or payload.name,
            order_index,
            payload.epoch_id,
        ),
    )
    vid = cur.lastrowid
    conn.commit()
    conn.close()
    result = {"visit_id": vid, "order_index": order_index}
    _record_visit_audit(
        soa_id,
        "create",
        vid,
        before=None,
        after={
            "id": vid,
            "name": payload.name,
            "raw_header": payload.raw_header or payload.name,
            "order_index": order_index,
            "epoch_id": payload.epoch_id,
        },
    )
    return result


@app.patch("/soa/{soa_id}/visits/{visit_id}")
def update_visit(soa_id: int, visit_id: int, payload: VisitUpdate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,raw_header,order_index,epoch_id FROM visit WHERE id=? AND soa_id=?",
        (visit_id, soa_id),
    )
    row = cur.fetchone()
    if not row:
        conn.close()
        raise HTTPException(404, "Visit not found")
    before = {
        "id": row[0],
        "name": row[1],
        "raw_header": row[2],
        "order_index": row[3],
        "epoch_id": row[4],
    }
    # Validate epoch if provided (allow clearing)
    if payload.epoch_id is not None:
        if payload.epoch_id is not None:
            cur.execute(
                "SELECT 1 FROM epoch WHERE id=? AND soa_id=?",
                (payload.epoch_id, soa_id),
            )
            if not cur.fetchone():
                conn.close()
                raise HTTPException(400, "Invalid epoch_id for this SOA")
    new_name = (
        (payload.name if payload.name is not None else before["name"]) or ""
    ).strip()
    new_raw_header = (
        (payload.raw_header if payload.raw_header is not None else before["raw_header"])
        or new_name
        or ""
    ).strip()
    new_epoch_id = (
        payload.epoch_id if payload.epoch_id is not None else before["epoch_id"]
    )
    cur.execute(
        "UPDATE visit SET name=?, raw_header=?, epoch_id=? WHERE id=?",
        (new_name or None, new_raw_header or None, new_epoch_id, visit_id),
    )
    conn.commit()
    cur.execute(
        "SELECT id,name,raw_header,order_index,epoch_id FROM visit WHERE id=?",
        (visit_id,),
    )
    r = cur.fetchone()
    conn.close()
    after = {
        "id": r[0],
        "name": r[1],
        "raw_header": r[2],
        "order_index": r[3],
        "epoch_id": r[4],
    }
    mutable = ["name", "raw_header", "epoch_id"]
    updated_fields = [f for f in mutable if before.get(f) != after.get(f)]
    _record_visit_audit(
        soa_id,
        "update",
        visit_id,
        before=before,
        after={**after, "updated_fields": updated_fields},
    )
    return {**after, "updated_fields": updated_fields}


@app.get("/soa/{soa_id}/visits/{visit_id}")
def get_visit(soa_id: int, visit_id: int):
    """Return metadata for a single visit (parity with epoch detail endpoint)."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,raw_header,order_index,epoch_id FROM visit WHERE id=? AND soa_id=?",
        (visit_id, soa_id),
    )
    row = cur.fetchone()
    conn.close()
    if not row:
        raise HTTPException(404, "Visit not found")
    return {
        "id": row[0],
        "soa_id": soa_id,
        "name": row[1],
        "raw_header": row[2],
        "order_index": row[3],
        "epoch_id": row[4],
    }


@app.post("/soa/{soa_id}/activities")
def add_activity(soa_id: int, payload: ActivityCreate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM activity WHERE soa_id=?", (soa_id,))
    order_index = cur.fetchone()[0] + 1
    cur.execute(
        "INSERT INTO activity (soa_id,name,order_index,activity_uid) VALUES (?,?,?,?)",
        (soa_id, payload.name, order_index, f"Activity_{order_index}"),
    )
    aid = cur.lastrowid
    conn.commit()
    conn.close()
    result = {
        "activity_id": aid,
        "order_index": order_index,
        "activity_uid": f"Activity_{order_index}",
    }
    _record_activity_audit(
        soa_id,
        "create",
        aid,
        before=None,
        after={
            "id": aid,
            "name": payload.name,
            "order_index": order_index,
            "activity_uid": f"Activity_{order_index}",
        },
    )
    return result


@app.patch("/soa/{soa_id}/activities/{activity_id}")
def update_activity(soa_id: int, activity_id: int, payload: ActivityUpdate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,order_index FROM activity WHERE id=? AND soa_id=?",
        (activity_id, soa_id),
    )
    row = cur.fetchone()
    if not row:
        conn.close()
        raise HTTPException(404, "Activity not found")
    before = {"id": row[0], "name": row[1], "order_index": row[2]}
    new_name = (
        (payload.name if payload.name is not None else before["name"]) or ""
    ).strip()
    cur.execute(
        "UPDATE activity SET name=? WHERE id=?", (new_name or None, activity_id)
    )
    conn.commit()
    cur.execute(
        "SELECT id,name,order_index FROM activity WHERE id=?",
        (activity_id,),
    )
    r = cur.fetchone()
    conn.close()
    after = {"id": r[0], "name": r[1], "order_index": r[2]}
    updated_fields = ["name"] if before["name"] != after["name"] else []
    _record_activity_audit(
        soa_id,
        "update",
        activity_id,
        before=before,
        after={**after, "updated_fields": updated_fields},
    )
    return {**after, "updated_fields": updated_fields}


@app.get("/soa/{soa_id}/activities/{activity_id}")
def get_activity(soa_id: int, activity_id: int):
    """Return metadata for a single activity (parity with epoch & visit detail endpoints)."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,order_index FROM activity WHERE id=? AND soa_id=?",
        (activity_id, soa_id),
    )
    row = cur.fetchone()
    conn.close()
    if not row:
        raise HTTPException(404, "Activity not found")
    return {"id": row[0], "soa_id": soa_id, "name": row[1], "order_index": row[2]}


@app.post("/soa/{soa_id}/epochs")
def add_epoch(soa_id: int, payload: EpochCreate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM epoch WHERE soa_id=?", (soa_id,))
    order_index = cur.fetchone()[0] + 1
    # Immutable sequence per study
    cur.execute("SELECT MAX(epoch_seq) FROM epoch WHERE soa_id=?", (soa_id,))
    row = cur.fetchone()
    next_seq = (row[0] or 0) + 1
    cur.execute(
        "INSERT INTO epoch (soa_id,name,order_index,epoch_seq,epoch_label,epoch_description) VALUES (?,?,?,?,?,?)",
        (
            soa_id,
            payload.name,
            order_index,
            next_seq,
            (payload.epoch_label or "").strip() or None,
            (payload.epoch_description or "").strip() or None,
        ),
    )
    eid = cur.lastrowid
    conn.commit()
    conn.close()
    result = {"epoch_id": eid, "order_index": order_index, "epoch_seq": next_seq}
    _record_epoch_audit(
        soa_id,
        "create",
        eid,
        before=None,
        after={
            "id": eid,
            "name": payload.name,
            "order_index": order_index,
            "epoch_seq": next_seq,
            "epoch_label": (payload.epoch_label or "").strip() or None,
            "epoch_description": (payload.epoch_description or "").strip() or None,
        },
    )
    return result


@app.get("/soa/{soa_id}/epochs")
def list_epochs(soa_id: int):
    """Return ordered list of epoch metadata for a study."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    rows = [
        {
            "id": r[0],
            "name": r[1],
            "order_index": r[2],
            "epoch_seq": r[3],
            "epoch_label": r[4],
            "epoch_description": r[5],
        }
        for r in cur.fetchall()
    ]
    conn.close()
    return {"soa_id": soa_id, "epochs": rows}


@app.get("/soa/{soa_id}/epochs/{epoch_id}")
def get_epoch(soa_id: int, epoch_id: int):
    """Return metadata for a single epoch."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE id=? AND soa_id=?",
        (epoch_id, soa_id),
    )
    row = cur.fetchone()
    conn.close()
    if not row:
        raise HTTPException(404, "Epoch not found")
    return {
        "id": row[0],
        "soa_id": soa_id,
        "name": row[1],
        "order_index": row[2],
        "epoch_seq": row[3],
        "epoch_label": row[4],
        "epoch_description": row[5],
    }


@app.post("/soa/{soa_id}/epochs/{epoch_id}/metadata")
def update_epoch_metadata(soa_id: int, epoch_id: int, payload: EpochUpdate):
    """Update mutable epoch metadata (name, label, description)."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM epoch WHERE id=? AND soa_id=?", (epoch_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Epoch not found")
    # Capture before state
    cur.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE id=?",
        (epoch_id,),
    )
    b = cur.fetchone()
    before = None
    if b:
        before = {
            "id": b[0],
            "name": b[1],
            "order_index": b[2],
            "epoch_seq": b[3],
            "epoch_label": b[4],
            "epoch_description": b[5],
        }
    sets: List[str] = []
    vals: List[Any] = []
    if payload.name is not None:
        sets.append("name=?")
        vals.append((payload.name or "").strip() or None)
    if payload.epoch_label is not None:
        sets.append("epoch_label=?")
        vals.append((payload.epoch_label or "").strip() or None)
    if payload.epoch_description is not None:
        sets.append("epoch_description=?")
        vals.append((payload.epoch_description or "").strip() or None)
    if sets:
        vals.append(epoch_id)
        cur.execute(f"UPDATE epoch SET {', '.join(sets)} WHERE id=?", vals)
        conn.commit()
    cur.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE id=?",
        (epoch_id,),
    )
    row = cur.fetchone()
    conn.close()
    after = {
        "id": row[0],
        "name": row[1],
        "order_index": row[2],
        "epoch_seq": row[3],
        "epoch_label": row[4],
        "epoch_description": row[5],
    }
    mutable = ["name", "epoch_label", "epoch_description"]
    updated_fields = [f for f in mutable if before and before.get(f) != after.get(f)]
    _record_epoch_audit(
        soa_id,
        "update",
        epoch_id,
        before=before,
        after={**after, "updated_fields": updated_fields},
    )
    return {**after, "updated_fields": updated_fields}


@app.post("/soa/{soa_id}/activities/{activity_id}/concepts")
def set_activity_concepts(soa_id: int, activity_id: int, payload: ConceptsUpdate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM activity WHERE id=? AND soa_id=?", (activity_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Activity not found")
    # Clear existing mappings
    cur.execute("DELETE FROM activity_concept WHERE activity_id=?", (activity_id,))
    concepts = fetch_biomedical_concepts()
    lookup = {c["code"]: c["title"] for c in concepts}
    inserted = 0
    for code in payload.concept_codes:
        ccode = code.strip()
        if not ccode:
            continue
        title = lookup.get(ccode, ccode)
        cur.execute(
            "INSERT INTO activity_concept (activity_id, concept_code, concept_title) VALUES (?,?,?)",
            (activity_id, ccode, title),
        )
        inserted += 1
    conn.commit()
    conn.close()
    return {"activity_id": activity_id, "concepts_set": inserted}


def _get_activity_concepts(activity_id: int):
    """Return list of concepts (immutable: stored snapshot)."""
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT concept_code, concept_title FROM activity_concept WHERE activity_id=?",
        (activity_id,),
    )
    rows = [{"code": c, "title": t} for c, t in cur.fetchall()]
    conn.close()
    return rows


@app.post(
    "/ui/soa/{soa_id}/activity/{activity_id}/concepts/add", response_class=HTMLResponse
)
def ui_add_activity_concept(
    request: Request, soa_id: int, activity_id: int, concept_code: str = Form(...)
):
    if not activity_id:
        raise HTTPException(400, "Missing activity_id")
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    code = concept_code.strip()
    if not code:
        raise HTTPException(400, "Empty concept_code")
    concepts = fetch_biomedical_concepts()
    lookup = {c["code"]: c["title"] for c in concepts}
    title = lookup.get(code, code)
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM activity WHERE id=? AND soa_id=?", (activity_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Activity not found")
    cur.execute(
        "SELECT 1 FROM activity_concept WHERE activity_id=? AND concept_code=?",
        (activity_id, code),
    )
    if not cur.fetchone():
        cur.execute(
            "INSERT INTO activity_concept (activity_id, concept_code, concept_title) VALUES (?,?,?)",
            (activity_id, code, title),
        )
        conn.commit()
    conn.close()
    selected = _get_activity_concepts(activity_id)
    html = templates.get_template("concepts_cell.html").render(
        request=request,
        soa_id=soa_id,
        activity_id=activity_id,
        concepts=concepts,
        selected_codes=[s["code"] for s in selected],
        selected_list=selected,
        edit=False,
    )
    return HTMLResponse(html)


@app.post(
    "/ui/soa/{soa_id}/activity/{activity_id}/concepts/remove",
    response_class=HTMLResponse,
)
def ui_remove_activity_concept(
    request: Request, soa_id: int, activity_id: int, concept_code: str = Form(...)
):
    if not activity_id:
        raise HTTPException(400, "Missing activity_id")
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    code = concept_code.strip()
    if not code:
        raise HTTPException(400, "Empty concept_code")
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "DELETE FROM activity_concept WHERE activity_id=? AND concept_code=?",
        (activity_id, code),
    )
    conn.commit()
    conn.close()
    concepts = fetch_biomedical_concepts()
    selected = _get_activity_concepts(activity_id)
    html = templates.get_template("concepts_cell.html").render(
        request=request,
        soa_id=soa_id,
        activity_id=activity_id,
        concepts=concepts,
        selected_codes=[s["code"] for s in selected],
        selected_list=selected,
        edit=False,
    )
    return HTMLResponse(html)


@app.post("/soa/{soa_id}/activities/bulk")
def add_activities_bulk(soa_id: int, payload: BulkActivities):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    names = [n.strip() for n in payload.names if n and n.strip()]
    if not names:
        return {"added": 0, "skipped": 0, "details": []}
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT name FROM activity WHERE soa_id=?", (soa_id,))
    existing = set(r[0].lower() for r in cur.fetchall())
    added = []
    skipped = []
    # get current count for order_index start
    cur.execute("SELECT COUNT(*) FROM activity WHERE soa_id=?", (soa_id,))
    count = cur.fetchone()[0]
    order_index = count
    for name in names:
        lname = name.lower()
        if lname in existing:
            skipped.append(name)
            continue
        order_index += 1
        cur.execute(
            "INSERT INTO activity (soa_id,name,order_index,activity_uid) VALUES (?,?,?,?)",
            (soa_id, name, order_index, f"Activity_{order_index}"),
        )
        added.append(name)
        existing.add(lname)
    conn.commit()
    conn.close()
    return {
        "added": len(added),
        "skipped": len(skipped),
        "details": {"added": added, "skipped": skipped},
    }


@app.post("/soa/{soa_id}/cells")
def set_cell(soa_id: int, payload: CellCreate):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    # Upsert semantics: find existing
    cur.execute(
        "SELECT id FROM cell WHERE soa_id=? AND visit_id=? AND activity_id=?",
        (soa_id, payload.visit_id, payload.activity_id),
    )
    row = cur.fetchone()
    # If blank status => delete existing cell (clear) and do not create new row
    if payload.status.strip() == "":
        if row:
            cur.execute("DELETE FROM cell WHERE id=?", (row[0],))
            cid = row[0]
            conn.commit()
            conn.close()
            return {"cell_id": cid, "status": "", "deleted": True}
        conn.close()
        return {"cell_id": None, "status": "", "deleted": False}
    if row:
        cur.execute("UPDATE cell SET status=? WHERE id=?", (payload.status, row[0]))
        cid = row[0]
    else:
        cur.execute(
            "INSERT INTO cell (soa_id, visit_id, activity_id, status) VALUES (?,?,?,?)",
            (soa_id, payload.visit_id, payload.activity_id, payload.status),
        )
        cid = cur.lastrowid
    conn.commit()
    conn.close()
    return {"cell_id": cid, "status": payload.status}


@app.get("/soa/{soa_id}/matrix")
def get_matrix(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    visits, activities, cells = _fetch_matrix(soa_id)
    return {"visits": visits, "activities": activities, "cells": cells}


@app.get("/soa/{soa_id}/export/xlsx")
def export_xlsx(soa_id: int, left: Optional[int] = None, right: Optional[int] = None):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    visits, activities, cells = _fetch_matrix(soa_id)
    if not visits or not activities:
        raise HTTPException(
            400, "Cannot export empty matrix (need visits and activities)"
        )
    headers, rows = _matrix_arrays(soa_id)
    # Build DataFrame, then inject Concepts column (second position)
    df = pd.DataFrame(rows, columns=["Activity"] + headers)
    # Fetch concepts only (immutable snapshot titles)
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT activity_id, concept_code, concept_title FROM activity_concept")
    concepts_map = {}
    for aid, code, title in cur.fetchall():
        concepts_map.setdefault(aid, {})[code] = title
    conn.close()
    visits, activities, _cells = _fetch_matrix(soa_id)
    activity_ids_in_order = [a["id"] for a in activities]
    # Build display strings using EffectiveTitle (override if present) and show code in parentheses
    concepts_strings = []
    for aid in activity_ids_in_order:
        cmap = concepts_map.get(aid, {})
        if not cmap:
            concepts_strings.append("")
            continue
        items = sorted(cmap.items(), key=lambda kv: kv[1].lower())
        concepts_strings.append(
            "; ".join([f"{title} ({code})" for code, title in items])
        )
    if len(concepts_strings) == len(df):
        df.insert(1, "Concepts", concepts_strings)
    # Build concept mappings sheet data
    mapping_rows = []
    for a in activities:
        aid = a["id"]
        cmap = concepts_map.get(aid, {})
        for code, title in cmap.items():
            mapping_rows.append([aid, a["name"], code, title])
    mapping_df = pd.DataFrame(
        mapping_rows,
        columns=["ActivityID", "ActivityName", "ConceptCode", "ConceptTitle"],
    )
    # Build rollback audit sheet data (optional)
    audit_rows = (
        _list_rollback_audit(soa_id) if "_list_rollback_audit" in globals() else []
    )
    audit_df = pd.DataFrame(audit_rows)
    if audit_df.empty:
        audit_df = pd.DataFrame(
            columns=[
                "id",
                "freeze_id",
                "performed_at",
                "visits_restored",
                "activities_restored",
                "cells_restored",
                "concepts_restored",
            ]
        )
    bio = io.BytesIO()
    # Prepare cover sheet metadata
    # Fetch study core metadata (name, study fields, created_at)
    conn_info = _connect()
    cur_info = conn_info.cursor()
    cur_info.execute(
        "SELECT name, created_at, study_id, study_label, study_description FROM soa WHERE id=?",
        (soa_id,),
    )
    info_row = cur_info.fetchone()
    conn_info.close()
    if info_row:
        soa_name_val, created_at_val, study_id_val, study_label_val, study_desc_val = (
            info_row
        )
    else:
        soa_name_val, created_at_val, study_id_val, study_label_val, study_desc_val = (
            f"SOA {soa_id}",
            None,
            None,
            None,
            None,
        )
    freezes = _list_freezes(soa_id)
    last_freeze_label = freezes[0]["version_label"] if freezes else None
    last_freeze_time = freezes[0]["created_at"] if freezes else None
    left_freeze = _get_freeze(soa_id, left) if left else None
    right_freeze = _get_freeze(soa_id, right) if right else None
    concept_mapping_count = len(mapping_rows)
    cell_count = len(cells)
    meta_rows = [
        ["Study ID", study_id_val or ""],
        ["Study Name", soa_name_val],
        ["Study Label", study_label_val or ""],
        ["Study Description", (study_desc_val or "")[:4000]],
        ["Created At", created_at_val or ""],
        ["Visit Count", str(len(visits))],
        ["Activity Count", str(len(activities))],
        ["Cell Count", str(cell_count)],
        ["Concept Mapping Count", str(concept_mapping_count)],
        ["Frozen Versions Count", str(len(freezes))],
        ["Latest Freeze Label", last_freeze_label or ""],
        ["Latest Freeze Time", last_freeze_time or ""],
    ]
    if left_freeze and right_freeze:
        meta_rows.extend(
            [
                ["Diff Left Label", left_freeze.get("version_label")],
                ["Diff Left Frozen At", left_freeze.get("created_at")],
                ["Diff Right Label", right_freeze.get("version_label")],
                ["Diff Right Frozen At", right_freeze.get("created_at")],
            ]
        )
    study_df = pd.DataFrame(meta_rows, columns=["Key", "Value"])
    # Optional concept diff sheet if left/right provided
    concept_diff_df = None
    if left and right:
        try:
            diff = _diff_freezes_limited(soa_id, left, right, limit=None)
            left_freeze = _get_freeze(soa_id, left)
            right_freeze = _get_freeze(soa_id, right)
            activity_name_lookup = {}
            if left_freeze:
                for a in left_freeze.get("snapshot", {}).get("activities", []):
                    if isinstance(a, dict):
                        activity_name_lookup[str(a.get("id"))] = a.get("name")
            if right_freeze:
                for a in right_freeze.get("snapshot", {}).get("activities", []):
                    if isinstance(a, dict):
                        activity_name_lookup[str(a.get("id"))] = a.get("name")
            diff_rows = []
            for ch in diff.get("concepts", []):
                aid = str(ch.get("activity_id"))
                aname = activity_name_lookup.get(aid, "")
                added = ", ".join(ch.get("added", []))
                removed = ", ".join(ch.get("removed", []))
                title_changes = "; ".join(
                    [
                        f"{tc['code']}: '{tc['old_title']}' -> '{tc['new_title']}'"
                        for tc in ch.get("title_changes", [])
                    ]
                )
                diff_rows.append([aid, aname, added, removed, title_changes])
            concept_diff_df = pd.DataFrame(
                diff_rows,
                columns=[
                    "ActivityID",
                    "ActivityName",
                    "AddedConceptCodes",
                    "RemovedConceptCodes",
                    "TitleChanges",
                ],
            )
            if concept_diff_df.empty:
                concept_diff_df = pd.DataFrame(
                    columns=[
                        "ActivityID",
                        "ActivityName",
                        "AddedConceptCodes",
                        "RemovedConceptCodes",
                        "TitleChanges",
                    ]
                )
        except Exception as e:
            # Provide an error sheet to highlight issue rather than failing entire export
            concept_diff_df = pd.DataFrame([[str(e)]], columns=["ConceptDiffError"])
    with pd.ExcelWriter(bio, engine="openpyxl") as writer:
        study_df.to_excel(writer, index=False, sheet_name="Study")
        df.to_excel(writer, index=False, sheet_name="SoA")
        mapping_df.to_excel(writer, index=False, sheet_name="ConceptMappings")
        audit_df.to_excel(writer, index=False, sheet_name="RollbackAudit")
        if concept_diff_df is not None:
            concept_diff_df.to_excel(writer, index=False, sheet_name="ConceptDiff")
    bio.seek(0)
    # Dynamic filename pattern: studyid_version.xlsx
    # Determine study_id and version context
    conn_meta = _connect()
    cur_meta = conn_meta.cursor()
    cur_meta.execute("SELECT study_id FROM soa WHERE id=?", (soa_id,))
    row_meta = cur_meta.fetchone()
    conn_meta.close()
    study_id_val = (row_meta[0] if row_meta else None) or f"soa{soa_id}"
    # Sanitize study_id for filename (keep alnum, '-', '_')

    safe_study = (
        _re.sub(r"[^A-Za-z0-9_-]+", "-", study_id_val.strip())[:80] or f"soa{soa_id}"
    )
    version_segment = ""
    if left and right:
        # Diff export: include both labels
        left_f = _get_freeze(soa_id, left)
        right_f = _get_freeze(soa_id, right)
        left_label = left_f.get("version_label") if left_f else f"v{left}"
        right_label = right_f.get("version_label") if right_f else f"v{right}"
        version_segment = f"{left_label}_vs_{right_label}"
    else:
        freezes = _list_freezes(soa_id)
        if freezes:
            version_segment = freezes[0]["version_label"] or f"v{freezes[0]['id']}"
        else:
            # No freezes yet: assume initial version number 1
            version_segment = "v1"
    safe_version = _re.sub(r"[^A-Za-z0-9._-]+", "-", version_segment)[:60]
    filename = f"{safe_study}_{safe_version}.xlsx"
    return StreamingResponse(
        bio,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={"Content-Disposition": f'attachment; filename="{filename}"'},
    )


@app.get("/soa/{soa_id}/normalized")
def get_normalized(soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    csv_path = _generate_wide_csv(soa_id)
    out_dir = os.path.join(NORMALIZED_ROOT, f"soa_{soa_id}")
    os.makedirs(out_dir, exist_ok=True)
    summary = normalize_soa(
        csv_path, out_dir, sqlite_path=os.path.join(out_dir, "soa.db")
    )
    return {"summary": summary, "artifacts_dir": out_dir}


@app.post("/soa/{soa_id}/matrix/import")
def import_matrix(soa_id: int, payload: MatrixImport):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    if not payload.visits:
        raise HTTPException(400, "visits list empty")
    if not payload.activities:
        raise HTTPException(400, "activities list empty")
    visit_count = len(payload.visits)
    # Validate statuses length for each activity
    for act in payload.activities:
        if len(act.statuses) != visit_count:
            raise HTTPException(
                400,
                f"Activity '{act.name}' statuses length {len(act.statuses)} != visits length {visit_count}",
            )
    conn = _connect()
    cur = conn.cursor()
    if payload.reset:
        cur.execute("DELETE FROM cell WHERE soa_id=?", (soa_id,))
        cur.execute("DELETE FROM visit WHERE soa_id=?", (soa_id,))
        cur.execute("DELETE FROM activity WHERE soa_id=?", (soa_id,))
    # Insert visits respecting order
    cur.execute("SELECT COUNT(*) FROM visit WHERE soa_id=?", (soa_id,))
    vstart = cur.fetchone()[0]
    v_index = vstart
    visit_id_map = []
    for v in payload.visits:
        v_index += 1
        cur.execute(
            "INSERT INTO visit (soa_id,name,raw_header,order_index) VALUES (?,?,?,?)",
            (soa_id, v.name, v.raw_header or v.name, v_index),
        )
        visit_id_map.append(cur.lastrowid)
    # Insert activities
    cur.execute("SELECT COUNT(*) FROM activity WHERE soa_id=?", (soa_id,))
    astart = cur.fetchone()[0]
    a_index = astart
    activity_id_map = []
    for a in payload.activities:
        a_index += 1
        cur.execute(
            "INSERT INTO activity (soa_id,name,order_index,activity_uid) VALUES (?,?,?,?)",
            (soa_id, a.name, a_index, f"Activity_{a_index}"),
        )
        activity_id_map.append(cur.lastrowid)
    # Insert cells
    for a_idx, a in enumerate(payload.activities):
        aid = activity_id_map[a_idx]
        for v_idx, status in enumerate(a.statuses):
            if status is None:
                status = ""
            status_str = str(status).strip()
            if status_str == "":
                continue
            vid = visit_id_map[v_idx]
            cur.execute(
                "INSERT INTO cell (soa_id, visit_id, activity_id, status) VALUES (?,?,?,?)",
                (soa_id, vid, aid, status_str),
            )
    conn.commit()
    conn.close()
    return {
        "visits_added": len(payload.visits),
        "activities_added": len(payload.activities),
        "cells_inserted": sum(
            1 for a in payload.activities for s in a.statuses if str(s).strip() != ""
        ),
    }


# --------------------- Deletion API Endpoints ---------------------


def _reindex(table: str, soa_id: int):
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        f"SELECT id FROM {table} WHERE soa_id=? ORDER BY order_index", (soa_id,)
    )
    ids = [r[0] for r in cur.fetchall()]
    for idx, _id in enumerate(ids, start=1):
        cur.execute(f"UPDATE {table} SET order_index=? WHERE id=?", (idx, _id))
    # Maintain activity_uid after any activity reindex
    if table == "activity":
        # Two-phase UID refresh to satisfy UNIQUE(soa_id, activity_uid) without transient collisions
        cur.execute(
            "UPDATE activity SET activity_uid = 'TMP_' || id WHERE soa_id=?",
            (soa_id,),
        )
        cur.execute(
            "UPDATE activity SET activity_uid = 'Activity_' || order_index WHERE soa_id=?",
            (soa_id,),
        )
    conn.commit()
    conn.close()


@app.delete("/soa/{soa_id}/visits/{visit_id}")
def delete_visit(soa_id: int, visit_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM visit WHERE id=? AND soa_id=?", (visit_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Visit not found")
    # cascade cells
    # Capture before for audit
    cur.execute(
        "SELECT id,name,raw_header,order_index,epoch_id FROM visit WHERE id=?",
        (visit_id,),
    )
    b = cur.fetchone()
    before = None
    if b:
        before = {
            "id": b[0],
            "name": b[1],
            "raw_header": b[2],
            "order_index": b[3],
            "epoch_id": b[4],
        }
    cur.execute("DELETE FROM cell WHERE soa_id=? AND visit_id=?", (soa_id, visit_id))
    cur.execute("DELETE FROM visit WHERE id=?", (visit_id,))
    conn.commit()
    conn.close()
    _reindex("visit", soa_id)
    _record_visit_audit(soa_id, "delete", visit_id, before=before, after=None)
    return {"deleted_visit_id": visit_id}


@app.delete("/soa/{soa_id}/activities/{activity_id}")
def delete_activity(soa_id: int, activity_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM activity WHERE id=? AND soa_id=?", (activity_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Activity not found")
    cur.execute(
        "SELECT id,name,order_index FROM activity WHERE id=?",
        (activity_id,),
    )
    b = cur.fetchone()
    before = None
    if b:
        before = {"id": b[0], "name": b[1], "order_index": b[2]}
    cur.execute(
        "DELETE FROM cell WHERE soa_id=? AND activity_id=?", (soa_id, activity_id)
    )
    cur.execute("DELETE FROM activity WHERE id=?", (activity_id,))
    conn.commit()
    conn.close()
    _reindex("activity", soa_id)
    _record_activity_audit(soa_id, "delete", activity_id, before=before, after=None)
    return {"deleted_activity_id": activity_id}


@app.delete("/soa/{soa_id}/epochs/{epoch_id}")
def delete_epoch(soa_id: int, epoch_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM epoch WHERE id=? AND soa_id=?", (epoch_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Epoch not found")
    cur.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE id=?",
        (epoch_id,),
    )
    b = cur.fetchone()
    before = None
    if b:
        before = {
            "id": b[0],
            "name": b[1],
            "order_index": b[2],
            "epoch_seq": b[3],
            "epoch_label": b[4],
            "epoch_description": b[5],
        }
    cur.execute("DELETE FROM epoch WHERE id=", (epoch_id,))
    conn.commit()
    conn.close()
    _reindex("epoch", soa_id)
    _record_epoch_audit(soa_id, "delete", epoch_id, before=before, after=None)
    return {"deleted_epoch_id": epoch_id}


# --------------------- HTML UI Endpoints ---------------------


@app.get("/", response_class=HTMLResponse)
def ui_index(request: Request):
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT id,name,created_at,study_id,study_label,study_description FROM soa ORDER BY id DESC"
    )
    rows = cur.fetchall()
    conn.close()
    return templates.TemplateResponse(
        "index.html",
        {
            "request": request,
            "soas": [
                {
                    "id": r[0],
                    "name": r[1],
                    "created_at": r[2],
                    "study_id": r[3],
                    "study_label": r[4],
                    "study_description": r[5],
                }
                for r in rows
            ],
        },
    )


@app.post("/ui/soa/create", response_class=HTMLResponse)
def ui_create_soa(
    request: Request,
    name: str = Form(...),
    study_id: Optional[str] = Form(None),
    study_label: Optional[str] = Form(None),
    study_description: Optional[str] = Form(None),
):
    conn = _connect()
    cur = conn.cursor()
    # Uniqueness check
    if study_id and study_id.strip():
        cur.execute("SELECT 1 FROM soa WHERE study_id=?", (study_id.strip(),))
        if cur.fetchone():
            conn.close()
            return HTMLResponse(
                "<script>alert('study_id already exists');window.location='/'</script>"
            )
    cur.execute(
        "INSERT INTO soa (name, created_at, study_id, study_label, study_description) VALUES (?,?,?,?,?)",
        (
            name,
            datetime.now(timezone.utc).isoformat(),
            (study_id or "").strip() or None,
            (study_label or "").strip() or None,
            (study_description or "").strip() or None,
        ),
    )
    sid = cur.lastrowid
    conn.commit()
    conn.close()
    return HTMLResponse(f"<script>window.location='/ui/soa/{sid}/edit';</script>")


@app.post("/ui/soa/{soa_id}/update_meta", response_class=HTMLResponse)
def ui_update_meta(
    request: Request,
    soa_id: int,
    study_id: Optional[str] = Form(None),
    study_label: Optional[str] = Form(None),
    study_description: Optional[str] = Form(None),
):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT study_id FROM soa WHERE id=?", (soa_id,))
    row = cur.fetchone()
    current_study_id = row[0] if row else None
    proposed = (study_id or "").strip()
    if proposed == "" and current_study_id:
        new_study_id = current_study_id  # preserve existing
    else:
        new_study_id = proposed or None
    if new_study_id:
        cur.execute(
            "SELECT id FROM soa WHERE study_id=? AND id<>?", (new_study_id, soa_id)
        )
        if cur.fetchone():
            conn.close()
            return HTMLResponse(
                "<script>alert('study_id already exists');window.location='/ui/soa/%d/edit';</script>"
                % soa_id
            )
    if not current_study_id and not new_study_id:
        conn.close()
        return HTMLResponse(
            "<script>alert('study_id is required');window.location='/ui/soa/%d/edit';</script>"
            % soa_id
        )
    cur.execute(
        "UPDATE soa SET study_id=?, study_label=?, study_description=? WHERE id=?",
        (
            new_study_id,
            (study_label or "").strip() or None,
            (study_description or "").strip() or None,
            soa_id,
        ),
    )
    conn.commit()
    conn.close()
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.get("/ui/soa/{soa_id}/edit", response_class=HTMLResponse)
def ui_edit(request: Request, soa_id: int):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    visits, activities, cells = _fetch_matrix(soa_id)
    # Epochs list
    conn_ep = _connect()
    cur_ep = conn_ep.cursor()
    cur_ep.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    epochs = [
        dict(
            id=r[0],
            name=r[1],
            order_index=r[2],
            epoch_seq=r[3],
            epoch_label=r[4],
            epoch_description=r[5],
        )
        for r in cur_ep.fetchall()
    ]
    conn_ep.close()
    # Elements list
    conn_el = _connect()
    cur_el = conn_el.cursor()
    cur_el.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE soa_id=? ORDER BY order_index",
        (soa_id,),
    )
    elements = [
        dict(
            id=r[0],
            name=r[1],
            label=r[2],
            description=r[3],
            testrl=r[4],
            teenrl=r[5],
            order_index=r[6],
            created_at=r[7],
        )
        for r in cur_el.fetchall()
    ]
    conn_el.close()
    # No pagination: use all activities
    activities_page = activities
    # Build cell lookup
    cell_map = {(c["visit_id"], c["activity_id"]): c["status"] for c in cells}
    concepts = fetch_biomedical_concepts()
    activity_ids = [a["id"] for a in activities_page]
    activity_concepts = {}
    if activity_ids:
        conn = _connect()
        cur = conn.cursor()
        placeholders = ",".join("?" for _ in activity_ids)
        cur.execute(
            f"SELECT activity_id, concept_code, concept_title FROM activity_concept WHERE activity_id IN ({placeholders})",
            activity_ids,
        )
        for aid, code, title in cur.fetchall():
            activity_concepts.setdefault(aid, []).append({"code": code, "title": title})
        conn.close()
    concepts_diag = {
        "count": len(_concept_cache.get("data") or []),
        "last_status": _concept_cache.get("last_status"),
        "last_error": _concept_cache.get("last_error"),
        "api_key_present": bool(_get_cdisc_api_key()),
        "override_present": bool(_get_concepts_override()),
        "skip_remote": os.environ.get("CDISC_SKIP_REMOTE") == "1",
    }
    fetched_at = _concept_cache.get("fetched_at")
    last_fetch_iso = None
    last_fetch_relative = None
    if fetched_at:
        dt = datetime.fromtimestamp(fetched_at, tz=timezone.utc)
        last_fetch_iso = dt.isoformat()
        # Simple relative string (seconds/minutes/hours)
        delta = datetime.now(timezone.utc) - dt
        secs = int(delta.total_seconds())
        if secs < 60:
            last_fetch_relative = f"{secs}s ago"
        elif secs < 3600:
            last_fetch_relative = f"{secs//60}m ago"
        else:
            last_fetch_relative = f"{secs//3600}h ago"
    freeze_list = _list_freezes(soa_id)
    last_frozen_at = freeze_list[0]["created_at"] if freeze_list else None
    # Study metadata for edit form
    conn_meta = _connect()
    cur_meta = conn_meta.cursor()
    cur_meta.execute(
        "SELECT study_id, study_label, study_description FROM soa WHERE id=?", (soa_id,)
    )
    meta_row = cur_meta.fetchone()
    conn_meta.close()
    study_meta = {
        "study_id": meta_row[0] if meta_row else None,
        "study_label": meta_row[1] if meta_row else None,
        "study_description": meta_row[2] if meta_row else None,
    }
    return templates.TemplateResponse(
        "edit.html",
        {
            "request": request,
            "soa_id": soa_id,
            "epochs": epochs,
            "visits": visits,
            "activities": activities_page,
            "elements": elements,
            "arms": _fetch_arms_for_edit(soa_id),
            "cell_map": cell_map,
            "concepts": concepts,
            "activity_concepts": activity_concepts,
            "concepts_empty": len(concepts) == 0,
            "concepts_diag": concepts_diag,
            "concepts_last_fetch_iso": last_fetch_iso,
            "concepts_last_fetch_relative": last_fetch_relative,
            "freezes": freeze_list,
            "freeze_count": len(freeze_list),
            "last_frozen_at": last_frozen_at,
            **study_meta,
        },
    )


@app.get("/ui/concepts", response_class=HTMLResponse)
def ui_concepts_list(request: Request):
    """Render table listing biomedical concepts (title + href)."""
    concepts = fetch_biomedical_concepts(force=True) or []
    rows = []
    for c in concepts:
        code = c.get("concept_code") or c.get("code")
        title = c.get("title") or c.get("concept_title") or c.get("name") or code
        href = (
            f"https://api.library.cdisc.org/api/cosmos/v2/mdr/bc/biomedicalconcepts/{code}"
            if code
            else None
        )
        rows.append({"code": code, "title": title, "href": href})
    subscription_key = os.environ.get("CDISC_SUBSCRIPTION_KEY") or _get_cdisc_api_key()
    return templates.TemplateResponse(
        "concepts_list.html",
        {
            "request": request,
            "rows": rows,
            "count": len(rows),
            "missing_key": subscription_key is None,
        },
    )


@app.get("/ui/sdtm/specializations", response_class=HTMLResponse)
def ui_sdtm_specializations_list(request: Request):
    """Render table listing SDTM dataset specializations (title + API link)."""
    packages = fetch_sdtm_specializations(force=True) or []
    rows = [
        {"title": p.get("title") or "(untitled)", "href": p.get("href")}
        for p in packages
    ]
    subscription_key = os.environ.get("CDISC_SUBSCRIPTION_KEY") or _get_cdisc_api_key()
    # Diagnostics from cache for visibility when no data appears
    last_status = _sdtm_specializations_cache.get("last_status")
    last_error = _sdtm_specializations_cache.get("last_error")
    last_url = _sdtm_specializations_cache.get("last_url")
    return templates.TemplateResponse(
        "sdtm_specializations.html",
        {
            "request": request,
            "rows": rows,
            "count": len(rows),
            "missing_key": subscription_key is None,
            "last_status": last_status,
            "last_error": last_error,
            "last_url": last_url,
        },
    )


@app.get("/ui/sdtm/specializations/{idx}", response_class=HTMLResponse)
def ui_sdtm_specialization_detail(idx: int, request: Request):
    """Detail page for a single SDTM dataset specialization.

    Lookup by index into the cached list (stable for current request lifecycle).
    Fetches raw JSON from the specialization's href and pretty-prints result.
    Handles missing index, absent href, network or JSON parse errors gracefully.
    """
    packages = fetch_sdtm_specializations(force=True) or []
    if idx < 0 or idx >= len(packages):
        raise HTTPException(status_code=404, detail="Specialization index out of range")
    spec = packages[idx]
    title = spec.get("title") or "(untitled)"
    href = spec.get("href")

    api_key = _get_cdisc_api_key()
    subscription_key = os.environ.get("CDISC_SUBSCRIPTION_KEY")
    unified_key = subscription_key or api_key
    headers: dict[str, str] = {}
    if unified_key:
        headers["Ocp-Apim-Subscription-Key"] = unified_key
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
        headers["api-key"] = api_key

    status = None
    error = None
    pretty_json = None
    raw_text_snippet = None
    if href:
        try:
            resp = requests.get(href, headers=headers, timeout=15)
            status = resp.status_code
            raw_text_snippet = resp.text[:500]
            if resp.status_code == 200:
                try:
                    data = resp.json()
                except ValueError:
                    error = "200 OK but response was not valid JSON"
                    data = None
                if data is not None:
                    try:
                        pretty_json = json.dumps(data, indent=2, sort_keys=True)
                    except Exception:
                        pretty_json = json.dumps(data, indent=2)
            else:
                error = f"HTTP {resp.status_code} retrieving specialization"
        except Exception as e:
            error = f"Fetch error: {e}"[:300]
    else:
        error = "No href available for this specialization entry."

    return templates.TemplateResponse(
        "sdtm_specialization_detail.html",
        {
            "request": request,
            "index": idx,
            "title": title,
            "href": href,
            "status": status,
            "error": error,
            "pretty_json": pretty_json,
            "raw_text_snippet": raw_text_snippet,
            "missing_key": unified_key is None,
            "total": len(packages),
        },
    )


@app.get("/ui/concepts/{code}", response_class=HTMLResponse)
def ui_concept_detail(code: str, request: Request):
    """Detail page for a single biomedical concept. Fetches concept JSON from CDISC Library API,
    extracts title, canonical href, parentBiomedicalConcept href (if any), and parentPackage href.
    """
    # Build concept API URL
    api_href = (
        f"https://api.library.cdisc.org/api/cosmos/v2/mdr/bc/biomedicalconcepts/{code}"
    )
    headers = {}
    api_key = _get_cdisc_api_key()
    subscription_key = os.environ.get("CDISC_SUBSCRIPTION_KEY")
    # Some deployments use a single key; if only one provided, reuse it for both header styles
    unified_key = subscription_key or api_key
    if unified_key:
        headers["Ocp-Apim-Subscription-Key"] = unified_key
    if api_key:
        headers["Authorization"] = f"Bearer {api_key}"
        headers["api-key"] = api_key
    concept_json = None
    parent_bc_href = None
    parent_pkg_href = None
    parent_bc_title = None
    status = None
    try:
        resp = requests.get(api_href, headers=headers, timeout=10)
        status = resp.status_code
        if resp.status_code == 200:
            concept_json = resp.json()
            # Extract parent biomedical concept link if present
            parent_bc_href = concept_json.get(
                "parentBiomedicalConcept"
            ) or concept_json.get("parent_biomedical_concept")
            if isinstance(parent_bc_href, dict):
                parent_bc_title = parent_bc_href.get("title") or parent_bc_href.get(
                    "name"
                )
                parent_bc_href = parent_bc_href.get("href") or parent_bc_href.get("url")
            # Extract parent package link
            parent_pkg_href = concept_json.get("parentPackage") or concept_json.get(
                "parent_package"
            )
            if isinstance(parent_pkg_href, dict):
                parent_pkg_href = parent_pkg_href.get("href") or parent_pkg_href.get(
                    "url"
                )
        else:
            concept_json = {"error": f"Upstream returned {resp.status_code}"}
    except Exception as e:  # pragma: no cover
        concept_json = {"error": f"Request failed: {e}"}
    title = None
    if concept_json:
        title = (
            concept_json.get("title")
            or concept_json.get("concept_title")
            or concept_json.get("name")
            or code
        )
    return templates.TemplateResponse(
        "concept_detail.html",
        {
            "request": request,
            "code": code,
            "title": title,
            "api_href": api_href,
            "parent_bc_href": parent_bc_href,
            "parent_bc_title": parent_bc_title,
            "parent_pkg_href": parent_pkg_href,
            "status": status,
            "raw": json.dumps(concept_json, indent=2) if concept_json else None,
            "missing_key": unified_key is None,
        },
    )


@app.post("/ui/soa/{soa_id}/add_visit", response_class=HTMLResponse)
def ui_add_visit(
    request: Request,
    soa_id: int,
    name: str = Form(...),
    raw_header: str = Form(""),
    epoch_id: Optional[int] = Form(None),
):
    add_visit(
        soa_id, VisitCreate(name=name, raw_header=raw_header or name, epoch_id=epoch_id)
    )
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/add_arm", response_class=HTMLResponse)
def ui_add_arm(
    request: Request,
    soa_id: int,
    name: str = Form(...),
    label: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    element_id: Optional[str] = Form(None),
):
    """Form handler to create a new Arm."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    # Accept blank/empty element selection gracefully. The form may submit "" which would 422 with Optional[int].
    eid = int(element_id) if element_id and element_id.strip().isdigit() else None
    payload = ArmCreate(name=name, label=label, description=description, element_id=eid)
    create_arm(soa_id, payload)
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/update_arm", response_class=HTMLResponse)
def ui_update_arm(
    request: Request,
    soa_id: int,
    arm_id: int = Form(...),
    name: Optional[str] = Form(None),
    label: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    element_id: Optional[str] = Form(None),
):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    # Coerce possible blank element selection to None; avoid 422 validation error from string "" into Optional[int].
    eid = int(element_id) if element_id and element_id.strip().isdigit() else None
    payload = ArmUpdate(name=name, label=label, description=description, element_id=eid)
    update_arm(soa_id, arm_id, payload)
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/delete_arm", response_class=HTMLResponse)
def ui_delete_arm(request: Request, soa_id: int, arm_id: int = Form(...)):
    delete_arm(soa_id, arm_id)
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/reorder_arms", response_class=HTMLResponse)
def ui_reorder_arms(request: Request, soa_id: int, order: str = Form("")):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    ids = [int(x) for x in order.split(",") if x.strip().isdigit()]
    if not ids:
        return HTMLResponse("Invalid order", status_code=400)
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM arm WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM arm WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(ids) - existing:
        conn.close()
        return HTMLResponse("Order contains invalid arm id", status_code=400)
    for idx, aid in enumerate(ids, start=1):
        cur.execute("UPDATE arm SET order_index=? WHERE id=?", (idx, aid))
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "arm", old_order, ids)
    _record_arm_audit(
        soa_id,
        "reorder",
        arm_id=None,
        before={"old_order": old_order},
        after={"new_order": ids},
    )
    return HTMLResponse("OK")


@app.post("/ui/soa/{soa_id}/add_element", response_class=HTMLResponse)
def ui_add_element(
    request: Request,
    soa_id: int,
    name: str = Form(...),
    label: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    testrl: Optional[str] = Form(None),
    teenrl: Optional[str] = Form(None),
):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    name = (name or "").strip()
    if not name:
        raise HTTPException(400, "Name required")
    conn = _connect()
    cur = conn.cursor()
    # Determine next order index
    cur.execute(
        "SELECT COALESCE(MAX(order_index),0) FROM element WHERE soa_id=?", (soa_id,)
    )
    next_ord = (cur.fetchone() or [0])[0] + 1
    now = datetime.utcnow().isoformat()
    cur.execute(
        """INSERT INTO element (soa_id,name,label,description,testrl,teenrl,order_index,created_at)
        VALUES (?,?,?,?,?,?,?,?)""",
        (
            soa_id,
            name,
            (label or "").strip() or None,
            (description or "").strip() or None,
            (testrl or "").strip() or None,
            (teenrl or "").strip() or None,
            next_ord,
            now,
        ),
    )
    eid = cur.lastrowid
    conn.commit()
    conn.close()
    _record_element_audit(
        soa_id,
        "create",
        eid,
        before=None,
        after={
            "id": eid,
            "name": name,
            "label": (label or "").strip() or None,
            "description": (description or "").strip() or None,
            "testrl": (testrl or "").strip() or None,
            "teenrl": (teenrl or "").strip() or None,
            "order_index": next_ord,
        },
    )
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/update_element", response_class=HTMLResponse)
def ui_update_element(
    request: Request,
    soa_id: int,
    element_id: int = Form(...),
    name: Optional[str] = Form(None),
    label: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    testrl: Optional[str] = Form(None),
    teenrl: Optional[str] = Form(None),
):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM element WHERE id=? AND soa_id=?", (element_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Element not found")
    # Capture before
    cur.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE id=?",
        (element_id,),
    )
    b = cur.fetchone()
    before = None
    if b:
        before = {
            "id": b[0],
            "name": b[1],
            "label": b[2],
            "description": b[3],
            "testrl": b[4],
            "teenrl": b[5],
            "order_index": b[6],
            "created_at": b[7],
        }
    cur.execute(
        "UPDATE element SET name=?, label=?, description=?, testrl=?, teenrl=? WHERE id=?",
        (
            (name or "").strip() or None,
            (label or "").strip() or None,
            (description or "").strip() or None,
            (testrl or "").strip() or None,
            (teenrl or "").strip() or None,
            element_id,
        ),
    )
    conn.commit()
    # Fetch after
    cur.execute(
        "SELECT id,name,label,description,testrl,teenrl,order_index,created_at FROM element WHERE id=?",
        (element_id,),
    )
    a = cur.fetchone()
    conn.close()
    after = {
        "id": a[0],
        "name": a[1],
        "label": a[2],
        "description": a[3],
        "testrl": a[4],
        "teenrl": a[5],
        "order_index": a[6],
        "created_at": a[7],
    }
    mutable_fields = ["name", "label", "description", "testrl", "teenrl"]
    updated_fields = [
        f for f in mutable_fields if before and before.get(f) != after.get(f)
    ]
    _record_element_audit(
        soa_id,
        "update",
        element_id,
        before=before,
        after={**after, "updated_fields": updated_fields},
    )
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/delete_element", response_class=HTMLResponse)
def ui_delete_element(request: Request, soa_id: int, element_id: int = Form(...)):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("DELETE FROM element WHERE id=? AND soa_id=?", (element_id, soa_id))
    conn.commit()
    conn.close()
    _record_element_audit(
        soa_id, "delete", element_id, before={"id": element_id}, after=None
    )
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/reorder_elements", response_class=HTMLResponse)
def ui_reorder_elements(request: Request, soa_id: int, order: str = Form("")):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    ids = [int(x) for x in order.split(",") if x.strip().isdigit()]
    if not ids:
        return HTMLResponse("Invalid order", status_code=400)
    conn = _connect()
    cur = conn.cursor()
    # Capture existing order BEFORE modifying
    cur.execute("SELECT id FROM element WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    # Validate membership
    cur.execute("SELECT id FROM element WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(ids) - existing:
        conn.close()
        return HTMLResponse("Order contains invalid element id", status_code=400)
    for idx, eid in enumerate(ids, start=1):
        cur.execute("UPDATE element SET order_index=? WHERE id=?", (idx, eid))
    conn.commit()
    conn.close()
    # Record audit with before/after order
    _record_element_audit(
        soa_id,
        "reorder",
        element_id=None,
        before={"old_order": old_order},
        after={"new_order": ids},
    )
    return HTMLResponse("OK")


@app.post("/ui/soa/{soa_id}/add_activity", response_class=HTMLResponse)
def ui_add_activity(request: Request, soa_id: int, name: str = Form(...)):
    add_activity(soa_id, ActivityCreate(name=name))
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/add_epoch", response_class=HTMLResponse)
def ui_add_epoch(
    request: Request,
    soa_id: int,
    name: str = Form(...),
    epoch_label: Optional[str] = Form(None),
    epoch_description: Optional[str] = Form(None),
):
    add_epoch(
        soa_id,
        EpochCreate(
            name=name,
            epoch_label=epoch_label or None,
            epoch_description=epoch_description or None,
        ),
    )
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/update_epoch", response_class=HTMLResponse)
def ui_update_epoch(
    request: Request,
    soa_id: int,
    epoch_id: int = Form(...),
    name: Optional[str] = Form(None),
    epoch_label: Optional[str] = Form(None),
    epoch_description: Optional[str] = Form(None),
):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    payload = EpochUpdate(
        name=name,
        epoch_label=epoch_label,
        epoch_description=epoch_description,
    )
    # Reuse API logic
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM epoch WHERE id=? AND soa_id=?", (epoch_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Epoch not found")
    conn.close()
    # Capture before
    conn_b = _connect()
    cur_b = conn_b.cursor()
    cur_b.execute(
        "SELECT id,name,order_index,epoch_seq,epoch_label,epoch_description FROM epoch WHERE id=?",
        (epoch_id,),
    )
    b = cur_b.fetchone()
    conn_b.close()
    before = None
    if b:
        before = {
            "id": b[0],
            "name": b[1],
            "order_index": b[2],
            "epoch_seq": b[3],
            "epoch_label": b[4],
            "epoch_description": b[5],
        }
    after_api = update_epoch_metadata(soa_id, epoch_id, payload)
    _record_epoch_audit(
        soa_id,
        "update",
        epoch_id,
        before=before,
        after=after_api,
    )
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post(
    "/ui/soa/{soa_id}/activity/{activity_id}/concepts", response_class=HTMLResponse
)
def ui_set_activity_concepts(
    request: Request,
    soa_id: int,
    activity_id: int,
    concept_codes: List[str] = Form([]),
):
    payload = ConceptsUpdate(concept_codes=list(dict.fromkeys(concept_codes)))
    set_activity_concepts(soa_id, activity_id, payload)
    # HTMX inline update support
    if request.headers.get("HX-Request") == "true":
        concepts = fetch_biomedical_concepts()
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            "SELECT concept_code, concept_title FROM activity_concept WHERE activity_id=?",
            (activity_id,),
        )
        selected = [{"code": c, "title": t} for c, t in cur.fetchall()]
        conn.close()
        html = templates.get_template("concepts_cell.html").render(
            request=request,
            soa_id=soa_id,
            activity_id=activity_id,
            concepts=concepts,
            selected_codes=[s["code"] for s in selected],
            selected_list=selected,
            edit=False,
        )
        return HTMLResponse(html)
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.get(
    "/ui/soa/{soa_id}/activity/{activity_id}/concepts_cell", response_class=HTMLResponse
)
def ui_activity_concepts_cell(
    request: Request, soa_id: int, activity_id: int, edit: int = 0
):
    # Defensive guard: if activity_id is somehow falsy (should not happen for valid int path param)
    # surface a clear 400 error rather than proceeding and causing confusing downstream behavior.
    if not activity_id:
        raise HTTPException(status_code=400, detail="Missing activity_id")
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    concepts = fetch_biomedical_concepts()
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT concept_code, concept_title FROM activity_concept WHERE activity_id=?",
        (activity_id,),
    )
    selected = [{"code": c, "title": t} for c, t in cur.fetchall()]
    conn.close()
    return HTMLResponse(
        templates.get_template("concepts_cell.html").render(
            request=request,
            soa_id=soa_id,
            activity_id=activity_id,
            concepts=concepts,
            selected_codes=[s["code"] for s in selected],
            selected_list=selected,
            edit=bool(edit),
        )
    )


@app.post("/ui/soa/{soa_id}/set_cell", response_class=HTMLResponse)
def ui_set_cell(
    request: Request,
    soa_id: int,
    visit_id: int = Form(...),
    activity_id: int = Form(...),
    status: str = Form("X"),
):
    result = set_cell(
        soa_id, CellCreate(visit_id=visit_id, activity_id=activity_id, status=status)
    )
    return HTMLResponse(result.get("status", ""))


@app.post("/ui/soa/{soa_id}/toggle_cell", response_class=HTMLResponse)
def ui_toggle_cell(
    request: Request,
    soa_id: int,
    visit_id: int = Form(...),
    activity_id: int = Form(...),
):
    """Toggle logic: blank -> X, X -> blank (delete row). Returns updated <td> snippet with next action encoded.
    This avoids stale hx-vals attributes after a partial swap."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    # Determine current status
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT status,id FROM cell WHERE soa_id=? AND visit_id=? AND activity_id=?",
        (soa_id, visit_id, activity_id),
    )
    row = cur.fetchone()
    if row and row[0] == "X":
        # clear
        cur.execute("DELETE FROM cell WHERE id=?", (row[1],))
        conn.commit()
        conn.close()
        current = ""
    elif row:
        # Any non-blank treated as blank visually, remove
        cur.execute("DELETE FROM cell WHERE id=?", (row[1],))
        conn.commit()
        conn.close()
        current = ""
    else:
        # create X
        cur.execute(
            "INSERT INTO cell (soa_id, visit_id, activity_id, status) VALUES (?,?,?,?)",
            (soa_id, visit_id, activity_id, "X"),
        )
        conn.commit()
        conn.close()
        current = "X"
    # Next status (for hx-vals) depends on current
    next_status = "X" if current == "" else ""
    cell_html = f'<td hx-post="/ui/soa/{soa_id}/toggle_cell" hx-vals=\'{{"visit_id": {visit_id}, "activity_id": {activity_id}}}\' hx-swap="outerHTML" class="cell">{current}</td>'
    return HTMLResponse(cell_html)


@app.post("/ui/soa/{soa_id}/delete_visit", response_class=HTMLResponse)
def ui_delete_visit(request: Request, soa_id: int, visit_id: int = Form(...)):
    delete_visit(soa_id, visit_id)
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/set_visit_epoch", response_class=HTMLResponse)
def ui_set_visit_epoch(
    request: Request,
    soa_id: int,
    visit_id: int = Form(...),
    epoch_id: Optional[int] = Form(None),
):
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM visit WHERE id=? AND soa_id=?", (visit_id, soa_id))
    if not cur.fetchone():
        conn.close()
        raise HTTPException(404, "Visit not found")
    # Validate epoch (allow clearing with None)
    if epoch_id is not None:
        cur.execute("SELECT 1 FROM epoch WHERE id=? AND soa_id=?", (epoch_id, soa_id))
        if not cur.fetchone():
            conn.close()
            raise HTTPException(400, "Invalid epoch_id for this SOA")
    cur.execute("UPDATE visit SET epoch_id=? WHERE id=?", (epoch_id, visit_id))
    conn.commit()
    conn.close()
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/delete_activity", response_class=HTMLResponse)
def ui_delete_activity(request: Request, soa_id: int, activity_id: int = Form(...)):
    delete_activity(soa_id, activity_id)
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/delete_epoch", response_class=HTMLResponse)
def ui_delete_epoch(request: Request, soa_id: int, epoch_id: int = Form(...)):
    delete_epoch(soa_id, epoch_id)
    return HTMLResponse(f"<script>window.location='/ui/soa/{soa_id}/edit';</script>")


@app.post("/ui/soa/{soa_id}/reorder_visits", response_class=HTMLResponse)
def ui_reorder_visits(request: Request, soa_id: int, order: str = Form("")):
    """Persist new visit ordering. 'order' is a comma-separated list of visit IDs in desired order."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    ids = [int(x) for x in order.split(",") if x.strip().isdigit()]
    if not ids:
        return HTMLResponse("Invalid order", status_code=400)
    conn = _connect()
    cur = conn.cursor()
    # Capture existing order BEFORE modifications
    cur.execute("SELECT id FROM visit WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    # Validate membership
    cur.execute("SELECT id FROM visit WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(ids) - existing:
        conn.close()
        return HTMLResponse("Order contains invalid visit id", status_code=400)
    # Apply new order indices
    for idx, vid in enumerate(ids, start=1):
        cur.execute("UPDATE visit SET order_index=? WHERE id=?", (idx, vid))
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "visit", old_order, ids)
    return HTMLResponse("OK")


@app.post("/ui/soa/{soa_id}/reorder_activities", response_class=HTMLResponse)
def ui_reorder_activities(request: Request, soa_id: int, order: str = Form("")):
    """Persist new activity ordering. 'order' is a comma-separated list of activity IDs in desired order."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    ids = [int(x) for x in order.split(",") if x.strip().isdigit()]
    if not ids:
        return HTMLResponse("Invalid order", status_code=400)
    conn = _connect()
    cur = conn.cursor()
    # Capture previous order
    cur.execute(
        "SELECT id FROM activity WHERE soa_id=? ORDER BY order_index", (soa_id,)
    )
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM activity WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(ids) - existing:
        conn.close()
        return HTMLResponse("Order contains invalid activity id", status_code=400)
    for idx, aid in enumerate(ids, start=1):
        cur.execute("UPDATE activity SET order_index=? WHERE id=?", (idx, aid))
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "activity", old_order, ids)
    return HTMLResponse("OK")


@app.post("/ui/soa/{soa_id}/reorder_epochs", response_class=HTMLResponse)
def ui_reorder_epochs(request: Request, soa_id: int, order: str = Form("")):
    """Persist new epoch ordering."""
    if not _soa_exists(soa_id):
        raise HTTPException(404, "SOA not found")
    ids = [int(x) for x in order.split(",") if x.strip().isdigit()]
    if not ids:
        return HTMLResponse("Invalid order", status_code=400)
    conn = _connect()
    cur = conn.cursor()
    cur.execute("SELECT id FROM epoch WHERE soa_id=? ORDER BY order_index", (soa_id,))
    old_order = [r[0] for r in cur.fetchall()]
    cur.execute("SELECT id FROM epoch WHERE soa_id=?", (soa_id,))
    existing = {r[0] for r in cur.fetchall()}
    if set(ids) - existing:
        conn.close()
        return HTMLResponse("Order contains invalid epoch id", status_code=400)
    for idx, eid in enumerate(ids, start=1):
        cur.execute("UPDATE epoch SET order_index=? WHERE id=?", (idx, eid))
    conn.commit()
    conn.close()
    _record_reorder_audit(soa_id, "epoch", old_order, ids)
    # Also record epoch-specific reorder audit for parity with JSON endpoint
    _record_epoch_audit(
        soa_id,
        "reorder",
        epoch_id=None,
        before={"old_order": old_order},
        after={"new_order": ids},
    )
    return HTMLResponse("OK")


# --------------------- DDF Terminology Load ---------------------
def _sanitize_column(name: str) -> str:
    """Sanitize Excel column header to safe SQLite identifier: lowercase, replace spaces & non-alnum with underscore, collapse repeats."""
    import re

    s = name.strip().lower()
    s = re.sub(r"[^a-z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    if not s:
        s = "col"
    return s


def load_ddf_terminology(
    file_path: str,
    sheet_name: str = "DDF Terminology 2025-09-26",
    source: str = "admin",
    original_filename: Optional[str] = None,
    file_hash: Optional[str] = None,
) -> dict:
    """Load DDF terminology Excel sheet into SQLite table `ddf_terminology`.
    Recreates table each time (drop + create) for schema drift tolerance.
    Records an audit entry in ddf_terminology_audit.
    Returns dict with columns and row count.
    """
    # Extract dataset date ONLY from sheet_name (must contain YYYY-MM-DD).
    _date_pattern = re.compile(r"(20\d{2}-\d{2}-\d{2})")
    m = _date_pattern.search(sheet_name or "")
    if not m:
        raise HTTPException(
            400,
            "Sheet name must contain dataset date YYYY-MM-DD (e.g. 'DDF Terminology 2025-09-26')",
        )
    dataset_date = m.group(1)
    if not os.path.exists(file_path):
        # audit error record
        _record_ddf_audit(
            file_path=file_path,
            sheet_name=sheet_name,
            row_count=0,
            column_count=0,
            columns_json="[]",
            source=source,
            file_hash=file_hash,
            error=f"File not found: {file_path}",
            dataset_date=dataset_date,
        )
        raise HTTPException(400, f"File not found: {file_path}")
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name, dtype=str)
    except Exception as e:
        _record_ddf_audit(
            file_path=file_path,
            sheet_name=sheet_name,
            row_count=0,
            column_count=0,
            columns_json="[]",
            source=source,
            file_hash=file_hash,
            error=f"Read error: {e}",
            dataset_date=dataset_date,
        )
        raise HTTPException(400, f"Failed reading Excel: {e}")
    if df.empty:
        _record_ddf_audit(
            file_path=file_path,
            sheet_name=sheet_name,
            row_count=0,
            column_count=0,
            columns_json="[]",
            source=source,
            file_hash=file_hash,
            error="Worksheet empty",
            dataset_date=dataset_date,
        )
        raise HTTPException(400, "Worksheet is empty")
    # Build sanitized headers, discarding any worksheet column that normalizes to 'dataset_date'.
    raw_cols = list(df.columns)
    pairs = []  # (raw, sanitized)
    seen = set()
    for c in raw_cols:
        sc = _sanitize_column(str(c))
        if sc == "dataset_date":
            continue  # drop original dataset_date worksheet column; we inject a single synthetic one sourced from sheet name
        base = sc
        i = 2
        while sc in seen:
            sc = f"{base}_{i}"
            i += 1
        seen.add(sc)
        pairs.append((c, sc))
    sanitized = [sc for _, sc in pairs]
    sanitized.append("dataset_date")  # single authoritative dataset date column
    cols_sql = ", ".join(f"{c} TEXT" for c in sanitized)
    conn = _connect()
    cur = conn.cursor()
    cur.execute("DROP TABLE IF EXISTS ddf_terminology")
    cur.execute(
        f"CREATE TABLE ddf_terminology (id INTEGER PRIMARY KEY AUTOINCREMENT, {cols_sql})"
    )
    df = df.fillna("")
    kept_raw_cols = [raw for raw, sc in pairs]
    base_records = [
        tuple(str(row[c]) for c in kept_raw_cols) for _, row in df.iterrows()
    ]
    # Append dataset_date value per row (same for all rows)
    records = [r + (dataset_date,) for r in base_records]
    placeholders = ",".join(["?"] * (len(kept_raw_cols) + 1))
    cur.executemany(
        f"INSERT INTO ddf_terminology ({','.join(sanitized)}) VALUES ({placeholders})",
        records,
    )
    # Indexes for faster search/filter
    try:
        cur.execute("CREATE INDEX IF NOT EXISTS idx_ddf_code ON ddf_terminology(code)")
        if "cdisc_submission_value" in sanitized:
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_ddf_submission ON ddf_terminology(cdisc_submission_value)"
            )
        if "codelist_name" in sanitized:
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_ddf_codelist_name ON ddf_terminology(codelist_name)"
            )
    except Exception as ie:  # pragma: no cover
        logger.warning("Failed creating DDF indexes: %s", ie)
    conn.commit()
    conn.close()
    # Audit success
    _record_ddf_audit(
        file_path=file_path,
        sheet_name=sheet_name,
        row_count=len(records),
        column_count=len(sanitized),
        columns_json=json.dumps(sanitized),
        source=source,
        file_hash=file_hash,
        error=None,
        original_filename=original_filename or os.path.basename(file_path),
        dataset_date=dataset_date,
    )
    return {"columns": sanitized, "row_count": len(records)}


@app.post("/admin/load_ddf_terminology")
def admin_load_ddf(
    file_path: Optional[str] = None, sheet_name: str = "DDF Terminology 2025-09-26"
):
    """Admin endpoint to (re)load DDF terminology Excel sheet into SQLite."""
    # Determine repo root (src/soa_builder/web/app.py -> ascend 3 levels to /src, then one more to project root)
    project_root = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    )
    candidates = [
        os.path.join(
            project_root, "files", "DDF_Terminology_2025-09-26.xls"
        ),  # correct location
        os.path.join(
            os.path.dirname(os.path.dirname(os.path.dirname(__file__))),
            "files",
            "DDF_Terminology_2025-09-26.xls",
        ),  # previous wrong path for backward compatibility
    ]
    # If explicit file_path provided, prefer it
    if file_path:
        fp = file_path
    else:
        fp = None
        for c in candidates:
            if os.path.exists(c):
                fp = c
                break
        if fp is None:
            raise HTTPException(
                400, f"DDF terminology file not found in candidates: {candidates}"
            )
    # compute file hash for audit
    try:
        import hashlib

        with open(fp, "rb") as fh:
            file_hash = hashlib.sha256(fh.read()).hexdigest()
    except Exception:
        file_hash = None
    result = load_ddf_terminology(
        fp,
        sheet_name=sheet_name,
        source="admin",
        original_filename=os.path.basename(fp),
        file_hash=file_hash,
    )
    return JSONResponse(
        {"ok": True, **result, "file_path": fp, "sheet_name": sheet_name}
    )


@app.get("/ddf/terminology")
def get_ddf_terminology(
    search: Optional[str] = None,
    code: Optional[str] = None,
    codelist_name: Optional[str] = None,
    codelist_code: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
):
    """Query DDF terminology rows.
    Parameters:
      - search: case-insensitive substring across selected text columns.
      - code: exact match on primary code column (overrides search if provided).
      - limit/offset: pagination controls (limit capped at 200).
    Returns JSON with total_count, matched_count, rows, applied_filters.
    """
    limit = max(1, min(limit, 200))
    offset = max(0, offset)
    conn = _connect()
    cur = conn.cursor()
    # Ensure table exists
    cur.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='ddf_terminology'"
    )
    if not cur.fetchone():
        conn.close()
        raise HTTPException(
            404,
            "ddf_terminology table not found (load via POST /admin/load_ddf_terminology)",
        )
    # Column discovery
    cur.execute("PRAGMA table_info(ddf_terminology)")
    cols = [r[1] for r in cur.fetchall() if r[1] != "id"]
    searchable = [
        c
        for c in cols
        if c
        in [
            "code",
            "cdisc_submission_value",
            "cdisc_definition",
            "cdisc_synonym_s",
            "nci_preferred_term",
            "codelist_name",
            "codelist_code",
        ]
    ]
    cur.execute("SELECT COUNT(*) FROM ddf_terminology")
    total_count = cur.fetchone()[0]
    params = []
    where = []
    if code:
        where.append("code = ?")
        params.append(code)
    if codelist_name:
        where.append("codelist_name = ?")
        params.append(codelist_name)
    if codelist_code:
        where.append("codelist_code = ?")
        params.append(codelist_code)
    if (not code) and search:
        pattern = f"%{search.lower()}%"
        like_clauses = [f"LOWER({c}) LIKE ?" for c in searchable]
        params.extend([pattern] * len(like_clauses))
        where.append("(" + " OR ".join(like_clauses) + ")")
    where_sql = " WHERE " + " AND ".join(where) if where else ""
    count_sql = f"SELECT COUNT(*) FROM ddf_terminology{where_sql}"
    cur.execute(count_sql, params)
    matched_count = cur.fetchone()[0]
    select_cols = ["id"] + cols
    select_sql = f"SELECT {', '.join(select_cols)} FROM ddf_terminology{where_sql} ORDER BY code LIMIT ? OFFSET ?"
    cur.execute(select_sql, params + [limit, offset])
    rows_raw = cur.fetchall()
    # Build dict rows
    rows = []
    for r in rows_raw:
        d = {}
        for idx, col in enumerate(select_cols):
            d[col] = r[idx]
        rows.append(d)
    conn.close()
    return {
        "total_count": total_count,
        "matched_count": matched_count,
        "limit": limit,
        "offset": offset,
        "filters": {
            "search": search,
            "code": code,
            "codelist_name": codelist_name,
            "codelist_code": codelist_code,
        },
        "columns": select_cols,
        "rows": rows,
    }


@app.get("/ui/ddf/terminology", response_class=HTMLResponse)
def ui_ddf_terminology(
    request: Request,
    search: Optional[str] = None,
    code: Optional[str] = None,
    codelist_name: Optional[str] = None,
    codelist_code: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
    uploaded: Optional[str] = None,
    error: Optional[str] = None,
):
    data = get_ddf_terminology(
        search=search,
        code=code,
        codelist_name=codelist_name,
        codelist_code=codelist_code,
        limit=limit,
        offset=offset,
    )
    return templates.TemplateResponse(
        "ddf_terminology.html",
        {
            "request": request,
            **data,
            "search": search or "",
            "code": code or "",
            "codelist_name": codelist_name or "",
            "codelist_code": codelist_code or "",
            "uploaded": uploaded,
            "error": error,
        },
    )


@app.post("/ui/ddf/terminology/upload", response_class=HTMLResponse)
def ui_ddf_upload(
    request: Request,
    sheet_name: str = Form("DDF Terminology 2025-09-26"),
    file: UploadFile = File(...),
):
    """Upload an XLS/XLSX file and reload ddf_terminology table. Redirects back with status message."""
    # Basic validation
    filename = file.filename or "uploaded.xls"
    if not (filename.lower().endswith(".xls") or filename.lower().endswith(".xlsx")):
        return HTMLResponse(
            f"<script>window.location='/ui/ddf/terminology?error=Unsupported+file+type';</script>",
            status_code=400,
        )
    try:
        import tempfile

        suffix = ".xls" if filename.lower().endswith(".xls") else ".xlsx"
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
        contents = file.file.read()
        tmp.write(contents)
        tmp.flush()
        tmp.close()
        # hash
        import hashlib

        file_hash = hashlib.sha256(contents).hexdigest()
        load_ddf_terminology(
            tmp.name,
            sheet_name=sheet_name,
            source="upload",
            original_filename=filename,
            file_hash=file_hash,
        )
        return HTMLResponse(
            "<script>window.location='/ui/ddf/terminology?uploaded=1';</script>"
        )
    except HTTPException as he:
        return HTMLResponse(
            f"<script>window.location='/ui/ddf/terminology?error={he.detail}';</script>",
            status_code=400,
        )
    except Exception as e:
        esc = str(e).replace("'", "").replace('"', "")
        return HTMLResponse(
            f"<script>window.location='/ui/ddf/terminology?error={esc}';</script>",
            status_code=500,
        )


def _record_ddf_audit(
    file_path: str,
    sheet_name: str,
    row_count: int,
    column_count: int,
    columns_json: str,
    source: str,
    file_hash: Optional[str],
    error: Optional[str],
    original_filename: Optional[str] = None,
    dataset_date: Optional[str] = None,
):
    """Insert audit row (create table if missing)."""
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            """CREATE TABLE IF NOT EXISTS ddf_terminology_audit (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                loaded_at TEXT NOT NULL,
                file_path TEXT,
                original_filename TEXT,
                sheet_name TEXT,
                row_count INTEGER,
                column_count INTEGER,
                columns_json TEXT,
                source TEXT,
                file_hash TEXT,
                error TEXT,
                dataset_date TEXT
            )"""
        )
        # Migration: ensure dataset_date column exists if table was created earlier without it.
        cur.execute("PRAGMA table_info(ddf_terminology_audit)")
        audit_cols = {r[1] for r in cur.fetchall()}
        if "dataset_date" not in audit_cols:
            try:
                cur.execute(
                    "ALTER TABLE ddf_terminology_audit ADD COLUMN dataset_date TEXT"
                )
            except Exception:
                pass
        cur.execute(
            "INSERT INTO ddf_terminology_audit (loaded_at,file_path,original_filename,sheet_name,row_count,column_count,columns_json,source,file_hash,error,dataset_date) VALUES (?,?,?,?,?,?,?,?,?,?,?)",
            (
                datetime.utcnow().isoformat(),
                file_path,
                original_filename,
                sheet_name,
                row_count,
                column_count,
                columns_json,
                source,
                file_hash,
                error,
                dataset_date,
            ),
        )
        # Index for future date filtering
        try:
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_ddf_audit_dataset_date ON ddf_terminology_audit(dataset_date)"
            )
        except Exception:
            pass
        conn.commit()
        conn.close()
    except Exception as e:  # pragma: no cover
        logger.warning("Failed recording DDF audit: %s", e)


def _get_ddf_sources() -> List[str]:
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='ddf_terminology_audit'"
    )
    if not cur.fetchone():
        conn.close()
        return []
    cur.execute(
        "SELECT DISTINCT source FROM ddf_terminology_audit WHERE source IS NOT NULL ORDER BY source"
    )
    sources = [r[0] for r in cur.fetchall()]
    conn.close()
    return sources


@app.get("/ddf/terminology/audit")
def get_ddf_audit(
    source: Optional[str] = None, start: Optional[str] = None, end: Optional[str] = None
):
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='ddf_terminology_audit'"
    )
    if not cur.fetchone():
        conn.close()
        return []
    where_clauses = []
    params: List[Any] = []

    # Validate date inputs (YYYY-MM-DD)
    def _valid_date(d: str) -> bool:
        try:
            datetime.strptime(d, "%Y-%m-%d")
            return True
        except Exception:
            return False

    if source:
        where_clauses.append("source = ?")
        params.append(source)
    if start and _valid_date(start):
        where_clauses.append("substr(loaded_at,1,10) >= ?")
        params.append(start)
    if end and _valid_date(end):
        where_clauses.append("substr(loaded_at,1,10) <= ?")
        params.append(end)
    where_sql = (" WHERE " + " AND ".join(where_clauses)) if where_clauses else ""
    cur.execute(
        f"SELECT id,loaded_at,original_filename,file_path,sheet_name,row_count,column_count,source,file_hash,error FROM ddf_terminology_audit{where_sql} ORDER BY id DESC",
        params,
    )
    rows = []
    for r in cur.fetchall():
        rows.append(
            {
                "id": r[0],
                "loaded_at": r[1],
                "original_filename": r[2],
                "file_path": r[3],
                "sheet_name": r[4],
                "row_count": r[5],
                "column_count": r[6],
                "source": r[7],
                "file_hash": r[8],
                "error": r[9],
            }
        )
    conn.close()
    return rows


@app.get("/ddf/terminology/audit/export.csv")
def export_ddf_audit_csv(
    source: Optional[str] = None, start: Optional[str] = None, end: Optional[str] = None
):
    rows = get_ddf_audit(source=source, start=start, end=end)
    import csv, io

    buf = io.StringIO()
    writer = csv.writer(buf)
    writer.writerow(
        [
            "id",
            "loaded_at",
            "source",
            "original_filename",
            "file_hash",
            "row_count",
            "column_count",
            "sheet_name",
            "error",
        ]
    )
    for r in rows:
        writer.writerow(
            [
                r["id"],
                r["loaded_at"],
                r["source"],
                r["original_filename"],
                r["file_hash"],
                r["row_count"],
                r["column_count"],
                r["sheet_name"],
                r["error"] or "",
            ]
        )
    csv_data = buf.getvalue()
    return Response(
        content=csv_data,
        media_type="text/csv",
        headers={
            "Content-Disposition": "attachment; filename=ddf_terminology_audit.csv"
        },
    )


@app.get("/ddf/terminology/audit/export.json")
def export_ddf_audit_json(
    source: Optional[str] = None, start: Optional[str] = None, end: Optional[str] = None
):
    return get_ddf_audit(source=source, start=start, end=end)


@app.get("/ui/ddf/terminology/audit", response_class=HTMLResponse)
def ui_ddf_audit(
    request: Request,
    source: Optional[str] = None,
    start: Optional[str] = None,
    end: Optional[str] = None,
):
    rows = get_ddf_audit(source=source, start=start, end=end)
    sources = _get_ddf_sources()
    return templates.TemplateResponse(
        "ddf_terminology_audit.html",
        {
            "request": request,
            "rows": rows,
            "count": len(rows),
            "sources": sources,
            "current_source": source or "",
            "start": start or "",
            "end": end or "",
        },
    )


# --------------------- Entry ---------------------

# --------------------- Protocol Terminology Support ---------------------


def load_protocol_terminology(
    file_path: str,
    sheet_name: str = "Protocol Terminology 2025-09-26",
    source: str = "admin",
    original_filename: Optional[str] = None,
    file_hash: Optional[str] = None,
) -> dict:
    """Load Protocol terminology Excel sheet into SQLite table `protocol_terminology`.
    Mirrors load_ddf_terminology: drop/create table, sanitize headers, create indexes, record audit.
    """
    # Extract dataset date ONLY from sheet_name (must contain YYYY-MM-DD).
    _date_pattern = re.compile(r"(20\d{2}-\d{2}-\d{2})")
    m = _date_pattern.search(sheet_name or "")
    if not m:
        raise HTTPException(
            400,
            "Sheet name must contain dataset date YYYY-MM-DD (e.g. 'Protocol Terminology 2025-09-26')",
        )
    dataset_date = m.group(1)
    if not os.path.exists(file_path):
        _record_protocol_audit(
            file_path=file_path,
            sheet_name=sheet_name,
            row_count=0,
            column_count=0,
            columns_json="[]",
            source=source,
            file_hash=file_hash,
            error=f"File not found: {file_path}",
            dataset_date=dataset_date,
        )
        raise HTTPException(400, f"File not found: {file_path}")
    try:
        df = pd.read_excel(file_path, sheet_name=sheet_name, dtype=str)
    except Exception as e:
        _record_protocol_audit(
            file_path=file_path,
            sheet_name=sheet_name,
            row_count=0,
            column_count=0,
            columns_json="[]",
            source=source,
            file_hash=file_hash,
            error=f"Read error: {e}",
            dataset_date=dataset_date,
        )
        raise HTTPException(400, f"Failed reading Excel: {e}")
    if df.empty:
        _record_protocol_audit(
            file_path=file_path,
            sheet_name=sheet_name,
            row_count=0,
            column_count=0,
            columns_json="[]",
            source=source,
            file_hash=file_hash,
            error="Worksheet empty",
            dataset_date=dataset_date,
        )
        raise HTTPException(400, "Worksheet is empty")
    raw_cols = list(df.columns)
    pairs = []  # (raw, sanitized)
    seen = set()
    for c in raw_cols:
        sc = re.sub(r"[^a-zA-Z0-9_]+", "_", c.strip().lower()).strip("_") or "col"
        if sc == "dataset_date":
            continue  # drop any existing dataset_date worksheet column
        base = sc
        i = 1
        while sc in seen:
            sc = f"{base}_{i}"
            i += 1
        seen.add(sc)
        pairs.append((c, sc))
    sanitized = [sc for _, sc in pairs]
    sanitized.append("dataset_date")
    conn = _connect()
    cur = conn.cursor()
    cur.execute("DROP TABLE IF EXISTS protocol_terminology")
    cur.execute(
        "CREATE TABLE protocol_terminology (id INTEGER PRIMARY KEY AUTOINCREMENT, "
        + ",".join(f"{c} TEXT" for c in sanitized)
        + ")"
    )
    kept_raw_cols = [raw for raw, sc in pairs]
    base_records = [
        tuple(str(row[c]) for c in kept_raw_cols) for _, row in df.iterrows()
    ]
    records = [r + (dataset_date,) for r in base_records]
    placeholders = ",".join(["?"] * (len(kept_raw_cols) + 1))
    cur.executemany(
        f"INSERT INTO protocol_terminology ({','.join(sanitized)}) VALUES ({placeholders})",
        records,
    )
    try:
        if "code" in sanitized:
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_protocol_code ON protocol_terminology(code)"
            )
        if "codelist_name" in sanitized:
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_protocol_codelist_name ON protocol_terminology(codelist_name)"
            )
    except Exception as ie:  # pragma: no cover
        logger.warning("Failed creating Protocol indexes: %s", ie)
    conn.commit()
    conn.close()
    _record_protocol_audit(
        file_path=file_path,
        sheet_name=sheet_name,
        row_count=len(records),
        column_count=len(sanitized),
        columns_json=json.dumps(sanitized),
        source=source,
        file_hash=file_hash,
        error=None,
        original_filename=original_filename or os.path.basename(file_path),
        dataset_date=dataset_date,
    )
    return {"columns": sanitized, "row_count": len(records)}


@app.post("/admin/load_protocol_terminology")
def admin_load_protocol(
    file_path: Optional[str] = None, sheet_name: str = "Protocol Terminology 2025-09-26"
):
    project_root = os.path.dirname(
        os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    )
    candidates = [
        os.path.join(project_root, "files", "Protocol_Terminology_2025-09-26.xls"),
    ]
    if file_path:
        fp = file_path
    else:
        fp = None
        for c in candidates:
            if os.path.exists(c):
                fp = c
                break
        if fp is None:
            raise HTTPException(
                400, f"Protocol terminology file not found in candidates: {candidates}"
            )
    try:
        import hashlib, pathlib

        with open(fp, "rb") as fh:
            file_hash = hashlib.sha256(fh.read()).hexdigest()
    except Exception:
        file_hash = None
    result = load_protocol_terminology(
        fp,
        sheet_name=sheet_name,
        source="admin",
        original_filename=os.path.basename(fp),
        file_hash=file_hash,
    )
    return JSONResponse(
        {"ok": True, **result, "file_path": fp, "sheet_name": sheet_name}
    )


@app.get("/protocol/terminology")
def get_protocol_terminology(
    search: Optional[str] = None,
    code: Optional[str] = None,
    codelist_name: Optional[str] = None,
    codelist_code: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
):
    limit = max(1, min(limit, 200))
    offset = max(0, offset)
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='protocol_terminology'"
    )
    if not cur.fetchone():
        conn.close()
        raise HTTPException(
            404,
            "protocol_terminology table not found (load via POST /admin/load_protocol_terminology)",
        )
    cur.execute("PRAGMA table_info(protocol_terminology)")
    cols = [r[1] for r in cur.fetchall() if r[1] != "id"]
    searchable = [
        c
        for c in cols
        if c
        in [
            "code",
            "cdisc_submission_value",
            "cdisc_definition",
            "cdisc_synonym_s",
            "nci_preferred_term",
            "codelist_name",
            "codelist_code",
        ]
    ]
    cur.execute("SELECT COUNT(*) FROM protocol_terminology")
    total_count = cur.fetchone()[0]
    params: List[Any] = []
    where = []
    if code:
        where.append("code = ?")
        params.append(code)
    if codelist_name:
        where.append("codelist_name = ?")
        params.append(codelist_name)
    if codelist_code:
        where.append("codelist_code = ?")
        params.append(codelist_code)
    if (not code) and search:
        pattern = f"%{search.lower()}%"
        like_clauses = [f"LOWER({c}) LIKE ?" for c in searchable]
        params.extend([pattern] * len(like_clauses))
        where.append("(" + " OR ".join(like_clauses) + ")")
    where_sql = " WHERE " + " AND ".join(where) if where else ""
    cur.execute(f"SELECT COUNT(*) FROM protocol_terminology{where_sql}", params)
    matched_count = cur.fetchone()[0]
    select_cols = ["id"] + cols
    cur.execute(
        f"SELECT {', '.join(select_cols)} FROM protocol_terminology{where_sql} ORDER BY code LIMIT ? OFFSET ?",
        params + [limit, offset],
    )
    rows_raw = cur.fetchall()
    rows = []
    for r in rows_raw:
        d = {}
        for idx, col in enumerate(select_cols):
            d[col] = r[idx]
        rows.append(d)
    conn.close()
    return {
        "total_count": total_count,
        "matched_count": matched_count,
        "limit": limit,
        "offset": offset,
        "filters": {
            "search": search,
            "code": code,
            "codelist_name": codelist_name,
            "codelist_code": codelist_code,
        },
        "columns": select_cols,
        "rows": rows,
    }


@app.get("/ui/protocol/terminology", response_class=HTMLResponse)
def ui_protocol_terminology(
    request: Request,
    search: Optional[str] = None,
    code: Optional[str] = None,
    codelist_name: Optional[str] = None,
    codelist_code: Optional[str] = None,
    limit: int = 50,
    offset: int = 0,
    uploaded: Optional[str] = None,
    error: Optional[str] = None,
):
    data = get_protocol_terminology(
        search=search,
        code=code,
        codelist_name=codelist_name,
        codelist_code=codelist_code,
        limit=limit,
        offset=offset,
    )
    return templates.TemplateResponse(
        "protocol_terminology.html",
        {
            "request": request,
            **data,
            "search": search or "",
            "code": code or "",
            "codelist_name": codelist_name or "",
            "codelist_code": codelist_code or "",
            "uploaded": uploaded,
            "error": error,
        },
    )


@app.post("/ui/protocol/terminology/upload", response_class=HTMLResponse)
def ui_protocol_upload(
    request: Request,
    sheet_name: str = Form("Protocol Terminology 2025-09-26"),
    file: UploadFile = File(...),
):
    filename = file.filename or "uploaded.xls"
    if not (filename.lower().endswith(".xls") or filename.lower().endswith(".xlsx")):
        return HTMLResponse(
            "<script>window.location='/ui/protocol/terminology?error=Unsupported+file+type';</script>",
            status_code=400,
        )
    try:
        import tempfile, hashlib

        suffix = ".xls" if filename.lower().endswith(".xls") else ".xlsx"
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=suffix)
        contents = file.file.read()
        tmp.write(contents)
        tmp.flush()
        tmp.close()
        file_hash = hashlib.sha256(contents).hexdigest()
        load_protocol_terminology(
            tmp.name,
            sheet_name=sheet_name,
            source="upload",
            original_filename=filename,
            file_hash=file_hash,
        )
        return HTMLResponse(
            "<script>window.location='/ui/protocol/terminology?uploaded=1';</script>"
        )
    except HTTPException as he:
        return HTMLResponse(
            f"<script>window.location='/ui/protocol/terminology?error={he.detail}';</script>",
            status_code=400,
        )
    except Exception as e:
        esc = str(e).replace("'", "").replace('"', "")
        return HTMLResponse(
            f"<script>window.location='/ui/protocol/terminology?error={esc}';</script>",
            status_code=500,
        )


def _record_protocol_audit(
    file_path: str,
    sheet_name: str,
    row_count: int,
    column_count: int,
    columns_json: str,
    source: str,
    file_hash: Optional[str],
    error: Optional[str],
    original_filename: Optional[str] = None,
    dataset_date: Optional[str] = None,
):
    try:
        conn = _connect()
        cur = conn.cursor()
        cur.execute(
            """CREATE TABLE IF NOT EXISTS protocol_terminology_audit (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            loaded_at TEXT NOT NULL,
            file_path TEXT,
            original_filename TEXT,
            sheet_name TEXT,
            row_count INTEGER,
            column_count INTEGER,
            columns_json TEXT,
            source TEXT,
            file_hash TEXT,
            error TEXT,
            dataset_date TEXT
        )"""
        )
        cur.execute("PRAGMA table_info(protocol_terminology_audit)")
        audit_cols = {r[1] for r in cur.fetchall()}
        if "dataset_date" not in audit_cols:
            try:
                cur.execute(
                    "ALTER TABLE protocol_terminology_audit ADD COLUMN dataset_date TEXT"
                )
            except Exception:
                pass
        cur.execute(
            "INSERT INTO protocol_terminology_audit (loaded_at,file_path,original_filename,sheet_name,row_count,column_count,columns_json,source,file_hash,error,dataset_date) VALUES (?,?,?,?,?,?,?,?,?,?,?)",
            (
                datetime.utcnow().isoformat(),
                file_path,
                original_filename,
                sheet_name,
                row_count,
                column_count,
                columns_json,
                source,
                file_hash,
                error,
                dataset_date,
            ),
        )
        try:
            cur.execute(
                "CREATE INDEX IF NOT EXISTS idx_protocol_audit_dataset_date ON protocol_terminology_audit(dataset_date)"
            )
        except Exception:
            pass
        conn.commit()
        conn.close()
    except Exception as e:
        logger.warning("Failed recording Protocol audit: %s", e)


def _get_protocol_sources() -> List[str]:
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='protocol_terminology_audit'"
    )
    if not cur.fetchone():
        conn.close()
        return []
    cur.execute(
        "SELECT DISTINCT source FROM protocol_terminology_audit WHERE source IS NOT NULL ORDER BY source"
    )
    sources = [r[0] for r in cur.fetchall()]
    conn.close()
    return sources


@app.get("/protocol/terminology/audit")
def get_protocol_audit(
    source: Optional[str] = None, start: Optional[str] = None, end: Optional[str] = None
):
    conn = _connect()
    cur = conn.cursor()
    cur.execute(
        "SELECT name FROM sqlite_master WHERE type='table' AND name='protocol_terminology_audit'"
    )
    if not cur.fetchone():
        conn.close()
        return []
    where_clauses = []
    params: List[Any] = []

    def _valid_date(d: str) -> bool:
        try:
            datetime.strptime(d, "%Y-%m-%d")
            return True
        except Exception:
            return False

    if source:
        where_clauses.append("source = ?")
        params.append(source)
    if start and _valid_date(start):
        where_clauses.append("substr(loaded_at,1,10) >= ?")
        params.append(start)
    if end and _valid_date(end):
        where_clauses.append("substr(loaded_at,1,10) <= ?")
        params.append(end)
    where_sql = (" WHERE " + " AND ".join(where_clauses)) if where_clauses else ""
    cur.execute(
        f"SELECT id,loaded_at,original_filename,file_path,sheet_name,row_count,column_count,source,file_hash,error FROM protocol_terminology_audit{where_sql} ORDER BY id DESC",
        params,
    )
    rows = []
    for r in cur.fetchall():
        rows.append(
            {
                "id": r[0],
                "loaded_at": r[1],
                "original_filename": r[2],
                "file_path": r[3],
                "sheet_name": r[4],
                "row_count": r[5],
                "column_count": r[6],
                "source": r[7],
                "file_hash": r[8],
                "error": r[9],
            }
        )
    conn.close()
    return rows


@app.get("/protocol/terminology/audit/export.csv")
def export_protocol_audit_csv(
    source: Optional[str] = None, start: Optional[str] = None, end: Optional[str] = None
):
    rows = get_protocol_audit(source=source, start=start, end=end)
    import csv, io

    buf = io.StringIO()
    writer = csv.writer(buf)
    writer.writerow(
        [
            "id",
            "loaded_at",
            "source",
            "original_filename",
            "file_hash",
            "row_count",
            "column_count",
            "sheet_name",
            "error",
        ]
    )
    for r in rows:
        writer.writerow(
            [
                r["id"],
                r["loaded_at"],
                r["source"],
                r["original_filename"],
                r["file_hash"],
                r["row_count"],
                r["column_count"],
                r["sheet_name"],
                r["error"] or "",
            ]
        )
    csv_data = buf.getvalue()
    return Response(
        content=csv_data,
        media_type="text/csv",
        headers={
            "Content-Disposition": "attachment; filename=protocol_terminology_audit.csv"
        },
    )


@app.get("/protocol/terminology/audit/export.json")
def export_protocol_audit_json(
    source: Optional[str] = None, start: Optional[str] = None, end: Optional[str] = None
):
    return get_protocol_audit(source=source, start=start, end=end)


@app.get("/ui/protocol/terminology/audit", response_class=HTMLResponse)
def ui_protocol_audit(
    request: Request,
    source: Optional[str] = None,
    start: Optional[str] = None,
    end: Optional[str] = None,
):
    rows = get_protocol_audit(source=source, start=start, end=end)
    sources = _get_protocol_sources()
    return templates.TemplateResponse(
        "protocol_terminology_audit.html",
        {
            "request": request,
            "rows": rows,
            "count": len(rows),
            "sources": sources,
            "current_source": source or "",
            "start": start or "",
            "end": end or "",
        },
    )


def main():  # pragma: no cover
    import uvicorn

    uvicorn.run("soa_builder.web.app:app", host="0.0.0.0", port=8000, reload=True)


if __name__ == "__main__":  # pragma: no cover
    main()
